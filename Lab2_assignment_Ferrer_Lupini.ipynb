{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTcaLZaZ-lfb"
      },
      "source": [
        "# DGM Lab 2 - Autoregressive models\n",
        "\n",
        "In this Lab session we will implement recurrent neural networks (RNNs) in PyTorch. We will look at how your data should be structured in order to train such RNNs, and how we can use the networks as generators of new sequences of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfaXF1hp_gKl"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "For the first part of this lab session, we will use textual data, since this will allow us to have faster training times, more efficient models and more enjoyable results. On the one hand, training RNNs on images is generally resource-heavy because of their high dimensionality and complex structure. On the other hand, we cannot just use vanilla RNN architectures for images, since these will produce unsatisfying results &ndash; you should rather resort to architectures should as [PixelRNN](https://arxiv.org/abs/1601.06759) or [DRAW](https://arxiv.org/abs/1502.04623), but these are challenging to train and out of scope for this lab session. And as will become clear later on in this course, autoregressive models are generally not the preferred family of models to generate images.\n",
        "\n",
        "We will use the data from (almost) all of **Shakespeare's plays**. We have already assembled them together in a single txt file by crawling [Project Gutenberg](https://www.gutenberg.org/). The entire file is approximately 6.1 MB large, and can be downloaded into the `content` folder of this Colab notebook by executing the following script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmnv4ihe-_l_",
        "outputId": "b4c9bc7b-3f38-4e1d-d902-42691c913029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-24 16:14:35--  https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/master/datasets/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6347705 (6.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   6.05M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-05-24 16:14:35 (84.3 MB/s) - ‘shakespeare.txt’ saved [6347705/6347705]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/master/datasets/shakespeare.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nierLQnhEYr3"
      },
      "source": [
        "In the first part of this lab session, we will train a so-called **character-level RNN**. This means that we will operate on the character level of the text, and not on the word level, i.e. a single token is a separate character in the text and we will generate the text one character at a time. We do this again to optimize the computational footprint and efficiency of the model, since there are much less unique characters than unique words in a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFrhqd3_FohB"
      },
      "source": [
        "### Assignment 1\n",
        "\n",
        "Since neural networks only operate on numerical data, we have to be able to convert our text into such numerical format.\n",
        "\n",
        "1. Calculate the length (in no. of characters) of the entire dataset.\n",
        "2. Create a collection of all the unique characters in the dataset and calculate its size.\n",
        "3. Inspect the collection of unique characters. Are there any strange or unwanted characters? Remove them from the collection.\n",
        "4. Create two data structures that can be used to map each unique character onto a unique integer index, and vice versa. These will be used to convert between a text and a sequence of numbers.\n",
        "5. Since the dataset is not that large, we will keep the entire dataset in memory for quick access. Store the data as a single numerical (NumPy) array, thereby making use of the char-to-index map you created before. Make sure the array stores integers and not floats.\n",
        "\n",
        "If you want to further increase the efficiency of the model (and reduce the data dimensionality), you can first convert the entire dataset to lowercase letters, but this is not obligatory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6jrXQzCj3_4",
        "outputId": "3a0dd86f-e9bf-4146-907b-f011babcb153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the dataset is: 6347703 characters\n",
            "The collection of unique characters is: {'p', '\\n', 'b', 'l', 'j', 'V', ':', 'v', 'T', '\"', '}', 'Y', '8', 'S', 'x', 'I', 'K', ';', 'Z', '?', ']', 'W', 'A', '5', 'P', 'c', '(', 's', 'G', 'E', 'M', 'd', '6', '[', '&', 'f', 'k', 't', 'C', '-', 'q', ' ', 'X', 'o', 'i', '4', 'Q', '1', 'H', '.', 'J', ')', 'w', '<', 'D', '3', 'n', 'z', 'h', 'r', 'm', '2', '7', 'a', '\\ufeff', 'R', '_', ',', 'F', 'U', '!', '$', '9', 'u', \"'\", 'e', '0', 'B', 'N', 'L', 'y', 'g', 'O'}\n",
            "The size of the unique character set is: 83\n",
            "The cleaned collection of unique characters is: {'4', 'Q', '1', 'p', '\\n', 'H', 'b', '.', 'l', 'J', ')', 'j', 'w', 'V', '<', ':', 'D', 'v', 'T', '3', '\"', '}', 'n', 'Y', 'z', '8', 'S', 'x', 'I', 'h', 'r', 'm', '2', 'K', '7', ';', 'Z', 'a', '?', ']', 'W', '\\ufeff', 'R', 'A', '5', 'P', '_', 'c', '(', ',', 's', 'G', 'E', 'M', 'd', 'F', '6', 'U', '[', '&', 'f', '!', '$', '9', 'k', 'u', 't', \"'\", 'e', '0', 'C', 'B', '-', 'N', 'q', ' ', 'X', 'L', 'o', 'i', 'y', 'g', 'O'}\n",
            "The size of the cleaned unique character set is: 83\n",
            "The numerical array shape is: (6347703,)\n",
            "First 20 elements of the numerical array: [41  2 56 69 19  4  4 43 77 77 26 75 40 52 77 77 75 18  5 43]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_dataset_length(file_path):\n",
        "    \"\"\"Calculates the length (in number of characters) of the entire dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        The length of the dataset in characters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            return len(text)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return -1\n",
        "\n",
        "\n",
        "def calculate_unique_chars(file_path):\n",
        "    \"\"\"\n",
        "    Creates a collection of all the unique characters in the dataset and calculates its size.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            unique_chars = set(text)\n",
        "            return unique_chars, len(unique_chars)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None, -1\n",
        "\n",
        "\n",
        "def remove_unwanted_chars(unique_chars):\n",
        "    unwanted_chars = {\n",
        "        '\\x0c',  # Form feed\n",
        "        '\\u200b' # Zero width space\n",
        "        '\\ufeff'\n",
        "        '$'\n",
        "\n",
        "    }\n",
        "    cleaned_chars = unique_chars - unwanted_chars\n",
        "    return cleaned_chars\n",
        "\n",
        "def create_char_mappings(chars):\n",
        "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "    return char_to_idx, idx_to_char\n",
        "\n",
        "def create_numerical_array(file_path, char_to_idx):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    numerical_data = np.array([char_to_idx[char] for char in text if char in char_to_idx], dtype=np.int32)\n",
        "    return numerical_data\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = 'shakespeare.txt'\n",
        "dataset_length = calculate_dataset_length(file_path)\n",
        "unique_chars, unique_chars_size = calculate_unique_chars(file_path)\n",
        "cleaned_unique_chars = remove_unwanted_chars(unique_chars)\n",
        "\n",
        "char_to_idx, idx_to_char = create_char_mappings(cleaned_unique_chars)\n",
        "numerical_array = create_numerical_array(file_path, char_to_idx)\n",
        "\n",
        "\n",
        "if dataset_length != -1:\n",
        "    print(f\"The length of the dataset is: {dataset_length} characters\")\n",
        "if unique_chars_size != -1:\n",
        "    print(f\"The collection of unique characters is: {unique_chars}\")\n",
        "    print(f\"The size of the unique character set is: {unique_chars_size}\")\n",
        "print(f\"The cleaned collection of unique characters is: {cleaned_unique_chars}\")\n",
        "print(f\"The size of the cleaned unique character set is: {len(cleaned_unique_chars)}\")\n",
        "print(f\"The numerical array shape is: {numerical_array.shape}\")\n",
        "print(f\"First 20 elements of the numerical array: {numerical_array[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aBB1N0wUG9G"
      },
      "source": [
        "## Representing and batching the data\n",
        "\n",
        "When we represent characters as integers, we implicitly define an ordering between the characters: character 0 is close to character 1, but farther away from character 35. This is unwanted: the distance between each character in the feature space should ideally be the same. For this reason we will use a so-called **one-hot encoding** of each character. This is a vector of all zeros, except for a 1 at the index position of the considered character. For example: if we have 4 unique characters, character 0 is one-hot encoded as `[1,0,0,0]`, character 1 as `[0,1,0,0]`, character 2 as `[0,0,1,0]` and character 3 as `[0,0,0,1]`.\n",
        "\n",
        "As you know, neural networks are (traditionally) trained with stochastic gradient descent. This means that the data must be delivered in batches at the input of the network. For sequential data this means that the data becomes 3-dimensional: if $B$ is the batch size, $T$ is the sequence length and $D$ is the data dimensionality, then each batch has shape $(B, T, N)$. Note that this immediately implies that within a batch, all sequences must have the same length $T$. Does is done by either:\n",
        "\n",
        " * Only taking chunks of length $T$ from the dataset to fill up the batch, or\n",
        " * Also taking chunks of length $\\leq T$, and filling up the remainder of the sequences with invalid data (padding).\n",
        "\n",
        "Since our entire dataset is essentially one long sequence, we can always sample chunks of length $T$, so we will take the (simpler) first approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbH56mYO5cFN"
      },
      "source": [
        "### Assignment 2\n",
        "\n",
        "Create a custom Dataset class for the Shakespeare data (see lab 1 for more details). For this purpose, write appropriate `__init__`, `__len__` and `__getitem__` methods. Make sure that you can specify the desired sequence length $T$ and data dimensionality $D$. You can take different approaches:\n",
        "\n",
        " * Divide the dataset in chunks of equal length $T$ e.g. by following the truncated backpropagation through-time (TBPTT) parameterizations (see lecture). You can then calculate how many sequences your dataset counts.\n",
        " * Return a random chunk whenever `__getitem__` is called. This is more versatile than the approach above and leads to smoother loss minimization, but you lose the notion of an \"epoch\". In this case, you can return whatever (large) number you want in the `__len__` method (e.g. `sys.maxsize` returns a large integer number).\n",
        "\n",
        "Remember that you also have to return the training target labels for a sequence in the `__getitem__` method! Think again about what the task is (\"predict the next character in a sequence\") and then decide what the labels should be. Again you have different possibilities (see lecture: single-loss training vs. multi-loss training). Let's pick **multi-loss training** for now: in that case the target labels should also be a sequence.\n",
        "\n",
        "Make sure that the input data is one-hot encoded (this is not needed for the target labels) and that your data has dtype float32 (single precision; which is the PyTorch default for neural network parameters)!\n",
        "\n",
        "Initialize an instance of a DataLoader and test your Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYW1x4v2q0n-",
        "outputId": "81610240-6aa0-40b9-a84a-a16e5807ab2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Input data shape: torch.Size([32, 10, 83])\n",
            "Target data shape: torch.Size([32, 10])\n",
            "Batch 2:\n",
            "Input data shape: torch.Size([32, 10, 83])\n",
            "Target data shape: torch.Size([32, 10])\n",
            "Batch 3:\n",
            "Input data shape: torch.Size([32, 10, 83])\n",
            "Target data shape: torch.Size([32, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length, char_to_idx):\n",
        "        super(ShakespeareDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.num_chars = len(char_to_idx)\n",
        "        self.num_sequences = (len(data) - 1) // sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.sequence_length\n",
        "        end_idx = start_idx + self.sequence_length\n",
        "        sequence = self.data[start_idx:end_idx]\n",
        "        target = self.data[start_idx+1:end_idx+1]\n",
        "\n",
        "        # One-hot encode the input sequence\n",
        "        one_hot_sequence = torch.zeros(self.sequence_length, self.num_chars, dtype=torch.float32)\n",
        "        for i, char_idx in enumerate(sequence):\n",
        "            one_hot_sequence[i, char_idx] = 1\n",
        "\n",
        "        return one_hot_sequence, torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "# Example usage\n",
        "sequence_length = 10\n",
        "\n",
        "dataset = ShakespeareDataset(numerical_array, sequence_length, char_to_idx)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Test the dataset\n",
        "for i, (input_data, target_data) in enumerate(dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(\"Input data shape:\", input_data.shape)\n",
        "    print(\"Target data shape:\", target_data.shape)\n",
        "    if i >= 2:  # Check the first 3 batches for demonstration\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sydo2crMm4g"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "We will now build the recurrent neural network in PyTorch that will be trained to predict the next character in a given sequence of characters. We will leverage GRU layers as the main building blocks, but you can experiment with other layers as well. You can find more information on Pytorch's recurrent layers on [https://pytorch.org/docs/master/nn.html#recurrent-layers](https://pytorch.org/docs/master/nn.html#recurrent-layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg-THtIoXBDM"
      },
      "source": [
        "### Assignment 3\n",
        "\n",
        "A diagram of the envisioned architecture is shown in the picture below. Implement this architecture by inheriting from `torch.nn.Module`, as was explained in Lab 1. Test your module by getting a batch of data, feed it trough the network, and check if a sensible output is produced (especially if you have implemented the softmax - see remark 2 below - check if your output is properly normalized).\n",
        "\n",
        "![Image of RNN architecture](./RNNarchitecture.png)\n",
        "\n",
        "The one-hot encoded input sequence is fed through two GRU layers with 128 hidden states. The outputs of the last GRU layer are fed through two dense layers. The first dense layer has a (leaky) ReLU output activation (but you can experiment with other ones as well). The final dense layer has an output dimensionality of $D$ and calculates a softmax over all possible characters. The parallel arrows indicate that each layer calculates an output at every time step. Of course, the dense layers do not have a recurrent nature and do not process a sequence in its entirety: each input of the dense layer is processed separately of the others and leads to its own output. But it was cleaner to draw the diagram this way.\n",
        "\n",
        "**Important remark 1**: The Pytorch recurrent layers have a `batch_first` argument that you ideally set to `True`. That way, the first data dimension is considered the batch dimension.\n",
        "\n",
        "**Important remark 2**: For the softmax nonlinearity, you can either choose to leave it out of the model during training and offload its computation to the CrossEntropyLoss object later (as done in Lab 1). Or you can choose to attach the log-softmax nonlinearity and make it part of the model. This will have implications on the loss function that will be used during training (see later). It's up to you, but please make sure that you know what you are doing!\n",
        "\n",
        "**Important remark 3**: The initial hidden state $\\mathbf{h}_0$ of your GRU layers is initialized as a vector of zeroes, as a PyTorch default. It is, however, possible to parameterize the initial hidden states and train these parameters; if you are interested in this, you can search through the documentation or on the internet how you can achieve this (not obligatory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdxV7tXhNG2U",
        "outputId": "fdc7e1fd-e277-497f-f5e5-65b6f3d4dffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([128, 100, 83])\n",
            "Sum of probabilities along last dim (should be ~1): tensor([[[0.0123, 0.0122, 0.0110,  ..., 0.0125, 0.0119, 0.0123],\n",
            "         [0.0122, 0.0122, 0.0110,  ..., 0.0126, 0.0119, 0.0123],\n",
            "         [0.0121, 0.0122, 0.0110,  ..., 0.0126, 0.0118, 0.0123],\n",
            "         ...,\n",
            "         [0.0120, 0.0122, 0.0111,  ..., 0.0127, 0.0117, 0.0123],\n",
            "         [0.0120, 0.0122, 0.0111,  ..., 0.0127, 0.0117, 0.0123],\n",
            "         [0.0120, 0.0122, 0.0111,  ..., 0.0127, 0.0117, 0.0123]],\n",
            "\n",
            "        [[0.0123, 0.0122, 0.0110,  ..., 0.0125, 0.0120, 0.0124],\n",
            "         [0.0122, 0.0122, 0.0110,  ..., 0.0125, 0.0119, 0.0124],\n",
            "         [0.0122, 0.0122, 0.0110,  ..., 0.0126, 0.0119, 0.0123],\n",
            "         ...,\n",
            "         [0.0122, 0.0122, 0.0112,  ..., 0.0125, 0.0117, 0.0122],\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0118, 0.0123],\n",
            "         [0.0122, 0.0120, 0.0112,  ..., 0.0124, 0.0118, 0.0123]],\n",
            "\n",
            "        [[0.0123, 0.0122, 0.0110,  ..., 0.0125, 0.0119, 0.0123],\n",
            "         [0.0122, 0.0122, 0.0110,  ..., 0.0126, 0.0119, 0.0123],\n",
            "         [0.0122, 0.0122, 0.0111,  ..., 0.0126, 0.0118, 0.0123],\n",
            "         ...,\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0117, 0.0124],\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0117, 0.0123],\n",
            "         [0.0122, 0.0121, 0.0112,  ..., 0.0125, 0.0117, 0.0123]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0123, 0.0122, 0.0110,  ..., 0.0124, 0.0120, 0.0124],\n",
            "         [0.0123, 0.0121, 0.0111,  ..., 0.0125, 0.0119, 0.0124],\n",
            "         [0.0122, 0.0121, 0.0111,  ..., 0.0125, 0.0119, 0.0124],\n",
            "         ...,\n",
            "         [0.0122, 0.0121, 0.0112,  ..., 0.0125, 0.0117, 0.0123],\n",
            "         [0.0122, 0.0121, 0.0112,  ..., 0.0125, 0.0117, 0.0123],\n",
            "         [0.0122, 0.0121, 0.0112,  ..., 0.0125, 0.0118, 0.0124]],\n",
            "\n",
            "        [[0.0124, 0.0122, 0.0110,  ..., 0.0125, 0.0120, 0.0124],\n",
            "         [0.0123, 0.0122, 0.0110,  ..., 0.0125, 0.0119, 0.0124],\n",
            "         [0.0122, 0.0122, 0.0111,  ..., 0.0125, 0.0118, 0.0124],\n",
            "         ...,\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0118, 0.0123],\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0118, 0.0123],\n",
            "         [0.0122, 0.0121, 0.0112,  ..., 0.0124, 0.0118, 0.0123]],\n",
            "\n",
            "        [[0.0124, 0.0122, 0.0110,  ..., 0.0124, 0.0120, 0.0124],\n",
            "         [0.0123, 0.0121, 0.0111,  ..., 0.0125, 0.0119, 0.0123],\n",
            "         [0.0123, 0.0121, 0.0111,  ..., 0.0125, 0.0118, 0.0124],\n",
            "         ...,\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0118, 0.0123],\n",
            "         [0.0121, 0.0121, 0.0112,  ..., 0.0125, 0.0118, 0.0124],\n",
            "         [0.0122, 0.0121, 0.0112,  ..., 0.0124, 0.0118, 0.0123]]],\n",
            "       device='cuda:0', grad_fn=<ExpBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru1 = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dense1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dense2 = nn.Linear(hidden_size, output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2) # Log-softmax for numerical stability\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states (zeros by default)\n",
        "        h0_gru1 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        h0_gru2 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # GRU layers\n",
        "        out_gru1, _ = self.gru1(x, h0_gru1)\n",
        "        out_gru2, _ = self.gru2(out_gru1, h0_gru2)\n",
        "\n",
        "        # Dense layers\n",
        "        out_dense1 = self.relu(self.dense1(out_gru2))\n",
        "        out_dense2 = self.dense2(out_dense1)\n",
        "        out = self.log_softmax(out_dense2) # Apply log-softmax\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example instantiation and testing\n",
        "input_size = len(cleaned_unique_chars)\n",
        "hidden_size = 128\n",
        "output_size = len(cleaned_unique_chars)\n",
        "\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Get a batch of data\n",
        "for batch_idx, (data, target) in enumerate(dataloader):\n",
        "    data = data.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Feed the data through the network\n",
        "    output = model(data)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    # Check output normalization (log-softmax)\n",
        "\n",
        "    print(\"Sum of probabilities along last dim (should be ~1):\", torch.exp(output))\n",
        "\n",
        "    break # Inspect only the first batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-HRn9RvrOMn"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now that we have the dataset and the model ready, it is time to train our very first character-level RNN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YbFy9Ex9gAX"
      },
      "source": [
        "### Assignment 4\n",
        "\n",
        "Write an optimization procedure to train the RNN from assignment 3 using the data and batching strategy from assignments 1 and 2. We advise you to use the **Adam** optimizer with learning rate 0.001, which has become one of the default optimizers in deep learning, especially if you don't want to spend much time figuring out an effective learning rate schedule for plain SGD (which generally can lead to better optimizations). Pick a large enough batch size, e.g. 128, and a sequence length $T$ of around 100. Around 50 epochs of 1000 batches should be enough for now to train this model until \"reasonable\" convergence. Make sure to put the model and each batch on the GPU.\n",
        "\n",
        "**Important remark**: those of you who left out the log-softmax from the model, will need to use a CrossEntropyLoss. If you did use a log-softmax, you need a NLLLoss. Please read the documentation carefully regarding the use of these loss functions. Since we use **multi-loss training** we have a classification target at each time step, i.e. $T$ different loss values for each entry in the batch. Find out how to *correctly(!)* combine these losses into a single number (they can be averaged or summed, but make sure this is done along the correct axis). Another option (instead of using the built-in loss functions) is to write the loss criterion yourself in a separate function; this can be a nice exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e27e9e8-7af8-4ec3-f435-63ebdddb3eda",
        "id": "Gjeeqau3ddrZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Batch [1/1000], Loss: 4.4450\n",
            "Epoch [1/50], Batch [101/1000], Loss: 3.1894\n",
            "Epoch [1/50], Batch [201/1000], Loss: 2.7972\n",
            "Epoch [1/50], Batch [301/1000], Loss: 2.3104\n",
            "Epoch [1/50], Batch [401/1000], Loss: 2.1366\n",
            "Epoch [1/50], Batch [501/1000], Loss: 1.9986\n",
            "Epoch [1/50], Batch [601/1000], Loss: 1.9903\n",
            "Epoch [1/50], Batch [701/1000], Loss: 1.9193\n",
            "Epoch [1/50], Batch [801/1000], Loss: 1.8410\n",
            "Epoch [1/50], Batch [901/1000], Loss: 1.8616\n",
            "Epoch [2/50], Batch [1/1000], Loss: 1.7894\n",
            "Epoch [2/50], Batch [101/1000], Loss: 1.7654\n",
            "Epoch [2/50], Batch [201/1000], Loss: 1.7016\n",
            "Epoch [2/50], Batch [301/1000], Loss: 1.7049\n",
            "Epoch [2/50], Batch [401/1000], Loss: 1.7143\n",
            "Epoch [2/50], Batch [501/1000], Loss: 1.6743\n",
            "Epoch [2/50], Batch [601/1000], Loss: 1.6255\n",
            "Epoch [2/50], Batch [701/1000], Loss: 1.6529\n",
            "Epoch [2/50], Batch [801/1000], Loss: 1.5805\n",
            "Epoch [2/50], Batch [901/1000], Loss: 1.6172\n",
            "Epoch [3/50], Batch [1/1000], Loss: 1.6049\n",
            "Epoch [3/50], Batch [101/1000], Loss: 1.5545\n",
            "Epoch [3/50], Batch [201/1000], Loss: 1.5705\n",
            "Epoch [3/50], Batch [301/1000], Loss: 1.5298\n",
            "Epoch [3/50], Batch [401/1000], Loss: 1.5499\n",
            "Epoch [3/50], Batch [501/1000], Loss: 1.5558\n",
            "Epoch [3/50], Batch [601/1000], Loss: 1.4915\n",
            "Epoch [3/50], Batch [701/1000], Loss: 1.4936\n",
            "Epoch [3/50], Batch [801/1000], Loss: 1.5172\n",
            "Epoch [3/50], Batch [901/1000], Loss: 1.5182\n",
            "Epoch [4/50], Batch [1/1000], Loss: 1.5084\n",
            "Epoch [4/50], Batch [101/1000], Loss: 1.4879\n",
            "Epoch [4/50], Batch [201/1000], Loss: 1.4846\n",
            "Epoch [4/50], Batch [301/1000], Loss: 1.4788\n",
            "Epoch [4/50], Batch [401/1000], Loss: 1.4575\n",
            "Epoch [4/50], Batch [501/1000], Loss: 1.4748\n",
            "Epoch [4/50], Batch [601/1000], Loss: 1.5019\n",
            "Epoch [4/50], Batch [701/1000], Loss: 1.4127\n",
            "Epoch [4/50], Batch [801/1000], Loss: 1.4785\n",
            "Epoch [4/50], Batch [901/1000], Loss: 1.4752\n",
            "Epoch [5/50], Batch [1/1000], Loss: 1.4193\n",
            "Epoch [5/50], Batch [101/1000], Loss: 1.4116\n",
            "Epoch [5/50], Batch [201/1000], Loss: 1.4544\n",
            "Epoch [5/50], Batch [301/1000], Loss: 1.4331\n",
            "Epoch [5/50], Batch [401/1000], Loss: 1.4241\n",
            "Epoch [5/50], Batch [501/1000], Loss: 1.4103\n",
            "Epoch [5/50], Batch [601/1000], Loss: 1.4108\n",
            "Epoch [5/50], Batch [701/1000], Loss: 1.4124\n",
            "Epoch [5/50], Batch [801/1000], Loss: 1.3776\n",
            "Epoch [5/50], Batch [901/1000], Loss: 1.4101\n",
            "Epoch [6/50], Batch [1/1000], Loss: 1.3953\n",
            "Epoch [6/50], Batch [101/1000], Loss: 1.3862\n",
            "Epoch [6/50], Batch [201/1000], Loss: 1.4000\n",
            "Epoch [6/50], Batch [301/1000], Loss: 1.3752\n",
            "Epoch [6/50], Batch [401/1000], Loss: 1.4004\n",
            "Epoch [6/50], Batch [501/1000], Loss: 1.3902\n",
            "Epoch [6/50], Batch [601/1000], Loss: 1.4008\n",
            "Epoch [6/50], Batch [701/1000], Loss: 1.3559\n",
            "Epoch [6/50], Batch [801/1000], Loss: 1.3759\n",
            "Epoch [6/50], Batch [901/1000], Loss: 1.3753\n",
            "Epoch [7/50], Batch [1/1000], Loss: 1.4400\n",
            "Epoch [7/50], Batch [101/1000], Loss: 1.3575\n",
            "Epoch [7/50], Batch [201/1000], Loss: 1.3853\n",
            "Epoch [7/50], Batch [301/1000], Loss: 1.3527\n",
            "Epoch [7/50], Batch [401/1000], Loss: 1.3780\n",
            "Epoch [7/50], Batch [501/1000], Loss: 1.4005\n",
            "Epoch [7/50], Batch [601/1000], Loss: 1.3581\n",
            "Epoch [7/50], Batch [701/1000], Loss: 1.3431\n",
            "Epoch [7/50], Batch [801/1000], Loss: 1.3631\n",
            "Epoch [7/50], Batch [901/1000], Loss: 1.3375\n",
            "Epoch [8/50], Batch [1/1000], Loss: 1.3619\n",
            "Epoch [8/50], Batch [101/1000], Loss: 1.3390\n",
            "Epoch [8/50], Batch [201/1000], Loss: 1.3496\n",
            "Epoch [8/50], Batch [301/1000], Loss: 1.3578\n",
            "Epoch [8/50], Batch [401/1000], Loss: 1.3425\n",
            "Epoch [8/50], Batch [501/1000], Loss: 1.3901\n",
            "Epoch [8/50], Batch [601/1000], Loss: 1.3470\n",
            "Epoch [8/50], Batch [701/1000], Loss: 1.3694\n",
            "Epoch [8/50], Batch [801/1000], Loss: 1.3541\n",
            "Epoch [8/50], Batch [901/1000], Loss: 1.3244\n",
            "Epoch [9/50], Batch [1/1000], Loss: 1.3452\n",
            "Epoch [9/50], Batch [101/1000], Loss: 1.3387\n",
            "Epoch [9/50], Batch [201/1000], Loss: 1.3179\n",
            "Epoch [9/50], Batch [301/1000], Loss: 1.3993\n",
            "Epoch [9/50], Batch [401/1000], Loss: 1.3485\n",
            "Epoch [9/50], Batch [501/1000], Loss: 1.3284\n",
            "Epoch [9/50], Batch [601/1000], Loss: 1.3381\n",
            "Epoch [9/50], Batch [701/1000], Loss: 1.3178\n",
            "Epoch [9/50], Batch [801/1000], Loss: 1.3352\n",
            "Epoch [9/50], Batch [901/1000], Loss: 1.3210\n",
            "Epoch [10/50], Batch [1/1000], Loss: 1.3025\n",
            "Epoch [10/50], Batch [101/1000], Loss: 1.3539\n",
            "Epoch [10/50], Batch [201/1000], Loss: 1.2910\n",
            "Epoch [10/50], Batch [301/1000], Loss: 1.3329\n",
            "Epoch [10/50], Batch [401/1000], Loss: 1.3254\n",
            "Epoch [10/50], Batch [501/1000], Loss: 1.3269\n",
            "Epoch [10/50], Batch [601/1000], Loss: 1.3793\n",
            "Epoch [10/50], Batch [701/1000], Loss: 1.3338\n",
            "Epoch [10/50], Batch [801/1000], Loss: 1.3176\n",
            "Epoch [10/50], Batch [901/1000], Loss: 1.3489\n",
            "Epoch [11/50], Batch [1/1000], Loss: 1.3129\n",
            "Epoch [11/50], Batch [101/1000], Loss: 1.3210\n",
            "Epoch [11/50], Batch [201/1000], Loss: 1.2988\n",
            "Epoch [11/50], Batch [301/1000], Loss: 1.3642\n",
            "Epoch [11/50], Batch [401/1000], Loss: 1.2934\n",
            "Epoch [11/50], Batch [501/1000], Loss: 1.3191\n",
            "Epoch [11/50], Batch [601/1000], Loss: 1.3084\n",
            "Epoch [11/50], Batch [701/1000], Loss: 1.3136\n",
            "Epoch [11/50], Batch [801/1000], Loss: 1.2683\n",
            "Epoch [11/50], Batch [901/1000], Loss: 1.3123\n",
            "Epoch [12/50], Batch [1/1000], Loss: 1.2675\n",
            "Epoch [12/50], Batch [101/1000], Loss: 1.3247\n",
            "Epoch [12/50], Batch [201/1000], Loss: 1.3140\n",
            "Epoch [12/50], Batch [301/1000], Loss: 1.3039\n",
            "Epoch [12/50], Batch [401/1000], Loss: 1.3213\n",
            "Epoch [12/50], Batch [501/1000], Loss: 1.3095\n",
            "Epoch [12/50], Batch [601/1000], Loss: 1.3199\n",
            "Epoch [12/50], Batch [701/1000], Loss: 1.2986\n",
            "Epoch [12/50], Batch [801/1000], Loss: 1.2936\n",
            "Epoch [12/50], Batch [901/1000], Loss: 1.3082\n",
            "Epoch [13/50], Batch [1/1000], Loss: 1.3063\n",
            "Epoch [13/50], Batch [101/1000], Loss: 1.3082\n",
            "Epoch [13/50], Batch [201/1000], Loss: 1.2960\n",
            "Epoch [13/50], Batch [301/1000], Loss: 1.2877\n",
            "Epoch [13/50], Batch [401/1000], Loss: 1.2977\n",
            "Epoch [13/50], Batch [501/1000], Loss: 1.3126\n",
            "Epoch [13/50], Batch [601/1000], Loss: 1.3021\n",
            "Epoch [13/50], Batch [701/1000], Loss: 1.2842\n",
            "Epoch [13/50], Batch [801/1000], Loss: 1.2880\n",
            "Epoch [13/50], Batch [901/1000], Loss: 1.2827\n",
            "Epoch [14/50], Batch [1/1000], Loss: 1.2979\n",
            "Epoch [14/50], Batch [101/1000], Loss: 1.2839\n",
            "Epoch [14/50], Batch [201/1000], Loss: 1.3144\n",
            "Epoch [14/50], Batch [301/1000], Loss: 1.2844\n",
            "Epoch [14/50], Batch [401/1000], Loss: 1.2783\n",
            "Epoch [14/50], Batch [501/1000], Loss: 1.2904\n",
            "Epoch [14/50], Batch [601/1000], Loss: 1.2726\n",
            "Epoch [14/50], Batch [701/1000], Loss: 1.2991\n",
            "Epoch [14/50], Batch [801/1000], Loss: 1.2593\n",
            "Epoch [14/50], Batch [901/1000], Loss: 1.2759\n",
            "Epoch [15/50], Batch [1/1000], Loss: 1.2771\n",
            "Epoch [15/50], Batch [101/1000], Loss: 1.3048\n",
            "Epoch [15/50], Batch [201/1000], Loss: 1.2680\n",
            "Epoch [15/50], Batch [301/1000], Loss: 1.3252\n",
            "Epoch [15/50], Batch [401/1000], Loss: 1.2740\n",
            "Epoch [15/50], Batch [501/1000], Loss: 1.2757\n",
            "Epoch [15/50], Batch [601/1000], Loss: 1.2970\n",
            "Epoch [15/50], Batch [701/1000], Loss: 1.2721\n",
            "Epoch [15/50], Batch [801/1000], Loss: 1.3176\n",
            "Epoch [15/50], Batch [901/1000], Loss: 1.2820\n",
            "Epoch [16/50], Batch [1/1000], Loss: 1.2857\n",
            "Epoch [16/50], Batch [101/1000], Loss: 1.2657\n",
            "Epoch [16/50], Batch [201/1000], Loss: 1.2898\n",
            "Epoch [16/50], Batch [301/1000], Loss: 1.3210\n",
            "Epoch [16/50], Batch [401/1000], Loss: 1.2893\n",
            "Epoch [16/50], Batch [501/1000], Loss: 1.2990\n",
            "Epoch [16/50], Batch [601/1000], Loss: 1.2606\n",
            "Epoch [16/50], Batch [701/1000], Loss: 1.2823\n",
            "Epoch [16/50], Batch [801/1000], Loss: 1.2846\n",
            "Epoch [16/50], Batch [901/1000], Loss: 1.2949\n",
            "Epoch [17/50], Batch [1/1000], Loss: 1.3165\n",
            "Epoch [17/50], Batch [101/1000], Loss: 1.2735\n",
            "Epoch [17/50], Batch [201/1000], Loss: 1.2898\n",
            "Epoch [17/50], Batch [301/1000], Loss: 1.2834\n",
            "Epoch [17/50], Batch [401/1000], Loss: 1.3199\n",
            "Epoch [17/50], Batch [501/1000], Loss: 1.2827\n",
            "Epoch [17/50], Batch [601/1000], Loss: 1.2761\n",
            "Epoch [17/50], Batch [701/1000], Loss: 1.2943\n",
            "Epoch [17/50], Batch [801/1000], Loss: 1.2574\n",
            "Epoch [17/50], Batch [901/1000], Loss: 1.2891\n",
            "Epoch [18/50], Batch [1/1000], Loss: 1.2910\n",
            "Epoch [18/50], Batch [101/1000], Loss: 1.2633\n",
            "Epoch [18/50], Batch [201/1000], Loss: 1.2943\n",
            "Epoch [18/50], Batch [301/1000], Loss: 1.2780\n",
            "Epoch [18/50], Batch [401/1000], Loss: 1.3096\n",
            "Epoch [18/50], Batch [501/1000], Loss: 1.2763\n",
            "Epoch [18/50], Batch [601/1000], Loss: 1.2657\n",
            "Epoch [18/50], Batch [701/1000], Loss: 1.2703\n",
            "Epoch [18/50], Batch [801/1000], Loss: 1.3051\n",
            "Epoch [18/50], Batch [901/1000], Loss: 1.2518\n",
            "Epoch [19/50], Batch [1/1000], Loss: 1.2522\n",
            "Epoch [19/50], Batch [101/1000], Loss: 1.2669\n",
            "Epoch [19/50], Batch [201/1000], Loss: 1.2952\n",
            "Epoch [19/50], Batch [301/1000], Loss: 1.2758\n",
            "Epoch [19/50], Batch [401/1000], Loss: 1.2561\n",
            "Epoch [19/50], Batch [501/1000], Loss: 1.2674\n",
            "Epoch [19/50], Batch [601/1000], Loss: 1.2832\n",
            "Epoch [19/50], Batch [701/1000], Loss: 1.2903\n",
            "Epoch [19/50], Batch [801/1000], Loss: 1.2809\n",
            "Epoch [19/50], Batch [901/1000], Loss: 1.2757\n",
            "Epoch [20/50], Batch [1/1000], Loss: 1.2552\n",
            "Epoch [20/50], Batch [101/1000], Loss: 1.3063\n",
            "Epoch [20/50], Batch [201/1000], Loss: 1.2784\n",
            "Epoch [20/50], Batch [301/1000], Loss: 1.2693\n",
            "Epoch [20/50], Batch [401/1000], Loss: 1.2806\n",
            "Epoch [20/50], Batch [501/1000], Loss: 1.2658\n",
            "Epoch [20/50], Batch [601/1000], Loss: 1.2687\n",
            "Epoch [20/50], Batch [701/1000], Loss: 1.2262\n",
            "Epoch [20/50], Batch [801/1000], Loss: 1.2749\n",
            "Epoch [20/50], Batch [901/1000], Loss: 1.2610\n",
            "Epoch [21/50], Batch [1/1000], Loss: 1.2678\n",
            "Epoch [21/50], Batch [101/1000], Loss: 1.2575\n",
            "Epoch [21/50], Batch [201/1000], Loss: 1.2613\n",
            "Epoch [21/50], Batch [301/1000], Loss: 1.2670\n",
            "Epoch [21/50], Batch [401/1000], Loss: 1.2944\n",
            "Epoch [21/50], Batch [501/1000], Loss: 1.2798\n",
            "Epoch [21/50], Batch [601/1000], Loss: 1.2463\n",
            "Epoch [21/50], Batch [701/1000], Loss: 1.2647\n",
            "Epoch [21/50], Batch [801/1000], Loss: 1.2674\n",
            "Epoch [21/50], Batch [901/1000], Loss: 1.2717\n",
            "Epoch [22/50], Batch [1/1000], Loss: 1.2434\n",
            "Epoch [22/50], Batch [101/1000], Loss: 1.2586\n",
            "Epoch [22/50], Batch [201/1000], Loss: 1.2833\n",
            "Epoch [22/50], Batch [301/1000], Loss: 1.2638\n",
            "Epoch [22/50], Batch [401/1000], Loss: 1.2564\n",
            "Epoch [22/50], Batch [501/1000], Loss: 1.2337\n",
            "Epoch [22/50], Batch [601/1000], Loss: 1.2640\n",
            "Epoch [22/50], Batch [701/1000], Loss: 1.2546\n",
            "Epoch [22/50], Batch [801/1000], Loss: 1.2339\n",
            "Epoch [22/50], Batch [901/1000], Loss: 1.2622\n",
            "Epoch [23/50], Batch [1/1000], Loss: 1.2548\n",
            "Epoch [23/50], Batch [101/1000], Loss: 1.2883\n",
            "Epoch [23/50], Batch [201/1000], Loss: 1.2370\n",
            "Epoch [23/50], Batch [301/1000], Loss: 1.2528\n",
            "Epoch [23/50], Batch [401/1000], Loss: 1.2532\n",
            "Epoch [23/50], Batch [501/1000], Loss: 1.2808\n",
            "Epoch [23/50], Batch [601/1000], Loss: 1.2791\n",
            "Epoch [23/50], Batch [701/1000], Loss: 1.2731\n",
            "Epoch [23/50], Batch [801/1000], Loss: 1.2672\n",
            "Epoch [23/50], Batch [901/1000], Loss: 1.2907\n",
            "Epoch [24/50], Batch [1/1000], Loss: 1.2670\n",
            "Epoch [24/50], Batch [101/1000], Loss: 1.2839\n",
            "Epoch [24/50], Batch [201/1000], Loss: 1.2368\n",
            "Epoch [24/50], Batch [301/1000], Loss: 1.2834\n",
            "Epoch [24/50], Batch [401/1000], Loss: 1.2735\n",
            "Epoch [24/50], Batch [501/1000], Loss: 1.2485\n",
            "Epoch [24/50], Batch [601/1000], Loss: 1.2416\n",
            "Epoch [24/50], Batch [701/1000], Loss: 1.2721\n",
            "Epoch [24/50], Batch [801/1000], Loss: 1.2809\n",
            "Epoch [24/50], Batch [901/1000], Loss: 1.2659\n",
            "Epoch [25/50], Batch [1/1000], Loss: 1.2586\n",
            "Epoch [25/50], Batch [101/1000], Loss: 1.2464\n",
            "Epoch [25/50], Batch [201/1000], Loss: 1.2742\n",
            "Epoch [25/50], Batch [301/1000], Loss: 1.2356\n",
            "Epoch [25/50], Batch [401/1000], Loss: 1.2276\n",
            "Epoch [25/50], Batch [501/1000], Loss: 1.2797\n",
            "Epoch [25/50], Batch [601/1000], Loss: 1.2464\n",
            "Epoch [25/50], Batch [701/1000], Loss: 1.2349\n",
            "Epoch [25/50], Batch [801/1000], Loss: 1.2561\n",
            "Epoch [25/50], Batch [901/1000], Loss: 1.2458\n",
            "Epoch [26/50], Batch [1/1000], Loss: 1.2582\n",
            "Epoch [26/50], Batch [101/1000], Loss: 1.2735\n",
            "Epoch [26/50], Batch [201/1000], Loss: 1.2582\n",
            "Epoch [26/50], Batch [301/1000], Loss: 1.2493\n",
            "Epoch [26/50], Batch [401/1000], Loss: 1.2453\n",
            "Epoch [26/50], Batch [501/1000], Loss: 1.2484\n",
            "Epoch [26/50], Batch [601/1000], Loss: 1.2572\n",
            "Epoch [26/50], Batch [701/1000], Loss: 1.2805\n",
            "Epoch [26/50], Batch [801/1000], Loss: 1.2410\n",
            "Epoch [26/50], Batch [901/1000], Loss: 1.2685\n",
            "Epoch [27/50], Batch [1/1000], Loss: 1.2532\n",
            "Epoch [27/50], Batch [101/1000], Loss: 1.2976\n",
            "Epoch [27/50], Batch [201/1000], Loss: 1.2571\n",
            "Epoch [27/50], Batch [301/1000], Loss: 1.2694\n",
            "Epoch [27/50], Batch [401/1000], Loss: 1.2524\n",
            "Epoch [27/50], Batch [501/1000], Loss: 1.2353\n",
            "Epoch [27/50], Batch [601/1000], Loss: 1.2506\n",
            "Epoch [27/50], Batch [701/1000], Loss: 1.2566\n",
            "Epoch [27/50], Batch [801/1000], Loss: 1.2501\n",
            "Epoch [27/50], Batch [901/1000], Loss: 1.2518\n",
            "Epoch [28/50], Batch [1/1000], Loss: 1.2694\n",
            "Epoch [28/50], Batch [101/1000], Loss: 1.2506\n",
            "Epoch [28/50], Batch [201/1000], Loss: 1.2658\n",
            "Epoch [28/50], Batch [301/1000], Loss: 1.2548\n",
            "Epoch [28/50], Batch [401/1000], Loss: 1.2488\n",
            "Epoch [28/50], Batch [501/1000], Loss: 1.2550\n",
            "Epoch [28/50], Batch [601/1000], Loss: 1.2675\n",
            "Epoch [28/50], Batch [701/1000], Loss: 1.2630\n",
            "Epoch [28/50], Batch [801/1000], Loss: 1.2603\n",
            "Epoch [28/50], Batch [901/1000], Loss: 1.2403\n",
            "Epoch [29/50], Batch [1/1000], Loss: 1.2447\n",
            "Epoch [29/50], Batch [101/1000], Loss: 1.2639\n",
            "Epoch [29/50], Batch [201/1000], Loss: 1.2705\n",
            "Epoch [29/50], Batch [301/1000], Loss: 1.2366\n",
            "Epoch [29/50], Batch [401/1000], Loss: 1.2232\n",
            "Epoch [29/50], Batch [501/1000], Loss: 1.2510\n",
            "Epoch [29/50], Batch [601/1000], Loss: 1.2382\n",
            "Epoch [29/50], Batch [701/1000], Loss: 1.2531\n",
            "Epoch [29/50], Batch [801/1000], Loss: 1.2640\n",
            "Epoch [29/50], Batch [901/1000], Loss: 1.2373\n",
            "Epoch [30/50], Batch [1/1000], Loss: 1.2582\n",
            "Epoch [30/50], Batch [101/1000], Loss: 1.2326\n",
            "Epoch [30/50], Batch [201/1000], Loss: 1.2305\n",
            "Epoch [30/50], Batch [301/1000], Loss: 1.2776\n",
            "Epoch [30/50], Batch [401/1000], Loss: 1.2380\n",
            "Epoch [30/50], Batch [501/1000], Loss: 1.2287\n",
            "Epoch [30/50], Batch [601/1000], Loss: 1.2613\n",
            "Epoch [30/50], Batch [701/1000], Loss: 1.2303\n",
            "Epoch [30/50], Batch [801/1000], Loss: 1.2344\n",
            "Epoch [30/50], Batch [901/1000], Loss: 1.2252\n",
            "Epoch [31/50], Batch [1/1000], Loss: 1.2542\n",
            "Epoch [31/50], Batch [101/1000], Loss: 1.2749\n",
            "Epoch [31/50], Batch [201/1000], Loss: 1.2439\n",
            "Epoch [31/50], Batch [301/1000], Loss: 1.2708\n",
            "Epoch [31/50], Batch [401/1000], Loss: 1.2140\n",
            "Epoch [31/50], Batch [501/1000], Loss: 1.2607\n",
            "Epoch [31/50], Batch [601/1000], Loss: 1.2552\n",
            "Epoch [31/50], Batch [701/1000], Loss: 1.2628\n",
            "Epoch [31/50], Batch [801/1000], Loss: 1.2535\n",
            "Epoch [31/50], Batch [901/1000], Loss: 1.2457\n",
            "Epoch [32/50], Batch [1/1000], Loss: 1.2488\n",
            "Epoch [32/50], Batch [101/1000], Loss: 1.2595\n",
            "Epoch [32/50], Batch [201/1000], Loss: 1.2128\n",
            "Epoch [32/50], Batch [301/1000], Loss: 1.2328\n",
            "Epoch [32/50], Batch [401/1000], Loss: 1.2740\n",
            "Epoch [32/50], Batch [501/1000], Loss: 1.2480\n",
            "Epoch [32/50], Batch [601/1000], Loss: 1.2218\n",
            "Epoch [32/50], Batch [701/1000], Loss: 1.2565\n",
            "Epoch [32/50], Batch [801/1000], Loss: 1.2507\n",
            "Epoch [32/50], Batch [901/1000], Loss: 1.2389\n",
            "Epoch [33/50], Batch [1/1000], Loss: 1.2385\n",
            "Epoch [33/50], Batch [101/1000], Loss: 1.2504\n",
            "Epoch [33/50], Batch [201/1000], Loss: 1.2458\n",
            "Epoch [33/50], Batch [301/1000], Loss: 1.2151\n",
            "Epoch [33/50], Batch [401/1000], Loss: 1.2240\n",
            "Epoch [33/50], Batch [501/1000], Loss: 1.2148\n",
            "Epoch [33/50], Batch [601/1000], Loss: 1.2406\n",
            "Epoch [33/50], Batch [701/1000], Loss: 1.2268\n",
            "Epoch [33/50], Batch [801/1000], Loss: 1.2476\n",
            "Epoch [33/50], Batch [901/1000], Loss: 1.2333\n",
            "Epoch [34/50], Batch [1/1000], Loss: 1.2459\n",
            "Epoch [34/50], Batch [101/1000], Loss: 1.2274\n",
            "Epoch [34/50], Batch [201/1000], Loss: 1.2214\n",
            "Epoch [34/50], Batch [301/1000], Loss: 1.2448\n",
            "Epoch [34/50], Batch [401/1000], Loss: 1.2202\n",
            "Epoch [34/50], Batch [501/1000], Loss: 1.1915\n",
            "Epoch [34/50], Batch [601/1000], Loss: 1.2318\n",
            "Epoch [34/50], Batch [701/1000], Loss: 1.2527\n",
            "Epoch [34/50], Batch [801/1000], Loss: 1.2428\n",
            "Epoch [34/50], Batch [901/1000], Loss: 1.2229\n",
            "Epoch [35/50], Batch [1/1000], Loss: 1.2173\n",
            "Epoch [35/50], Batch [101/1000], Loss: 1.2452\n",
            "Epoch [35/50], Batch [201/1000], Loss: 1.2600\n",
            "Epoch [35/50], Batch [301/1000], Loss: 1.2226\n",
            "Epoch [35/50], Batch [401/1000], Loss: 1.2432\n",
            "Epoch [35/50], Batch [501/1000], Loss: 1.2242\n",
            "Epoch [35/50], Batch [601/1000], Loss: 1.2132\n",
            "Epoch [35/50], Batch [701/1000], Loss: 1.2167\n",
            "Epoch [35/50], Batch [801/1000], Loss: 1.2413\n",
            "Epoch [35/50], Batch [901/1000], Loss: 1.2219\n",
            "Epoch [36/50], Batch [1/1000], Loss: 1.2165\n",
            "Epoch [36/50], Batch [101/1000], Loss: 1.2297\n",
            "Epoch [36/50], Batch [201/1000], Loss: 1.2383\n",
            "Epoch [36/50], Batch [301/1000], Loss: 1.2225\n",
            "Epoch [36/50], Batch [401/1000], Loss: 1.2742\n",
            "Epoch [36/50], Batch [501/1000], Loss: 1.2415\n",
            "Epoch [36/50], Batch [601/1000], Loss: 1.2021\n",
            "Epoch [36/50], Batch [701/1000], Loss: 1.2242\n",
            "Epoch [36/50], Batch [801/1000], Loss: 1.2637\n",
            "Epoch [36/50], Batch [901/1000], Loss: 1.2406\n",
            "Epoch [37/50], Batch [1/1000], Loss: 1.2412\n",
            "Epoch [37/50], Batch [101/1000], Loss: 1.2699\n",
            "Epoch [37/50], Batch [201/1000], Loss: 1.2249\n",
            "Epoch [37/50], Batch [301/1000], Loss: 1.2403\n",
            "Epoch [37/50], Batch [401/1000], Loss: 1.2332\n",
            "Epoch [37/50], Batch [501/1000], Loss: 1.2586\n",
            "Epoch [37/50], Batch [601/1000], Loss: 1.2359\n",
            "Epoch [37/50], Batch [701/1000], Loss: 1.2332\n",
            "Epoch [37/50], Batch [801/1000], Loss: 1.2175\n",
            "Epoch [37/50], Batch [901/1000], Loss: 1.1901\n",
            "Epoch [38/50], Batch [1/1000], Loss: 1.2167\n",
            "Epoch [38/50], Batch [101/1000], Loss: 1.2165\n",
            "Epoch [38/50], Batch [201/1000], Loss: 1.2262\n",
            "Epoch [38/50], Batch [301/1000], Loss: 1.2418\n",
            "Epoch [38/50], Batch [401/1000], Loss: 1.2473\n",
            "Epoch [38/50], Batch [501/1000], Loss: 1.2288\n",
            "Epoch [38/50], Batch [601/1000], Loss: 1.2431\n",
            "Epoch [38/50], Batch [701/1000], Loss: 1.2317\n",
            "Epoch [38/50], Batch [801/1000], Loss: 1.2331\n",
            "Epoch [38/50], Batch [901/1000], Loss: 1.2589\n",
            "Epoch [39/50], Batch [1/1000], Loss: 1.2393\n",
            "Epoch [39/50], Batch [101/1000], Loss: 1.2292\n",
            "Epoch [39/50], Batch [201/1000], Loss: 1.2644\n",
            "Epoch [39/50], Batch [301/1000], Loss: 1.2229\n",
            "Epoch [39/50], Batch [401/1000], Loss: 1.2286\n",
            "Epoch [39/50], Batch [501/1000], Loss: 1.2193\n",
            "Epoch [39/50], Batch [601/1000], Loss: 1.2642\n",
            "Epoch [39/50], Batch [701/1000], Loss: 1.2414\n",
            "Epoch [39/50], Batch [801/1000], Loss: 1.2491\n",
            "Epoch [39/50], Batch [901/1000], Loss: 1.2235\n",
            "Epoch [40/50], Batch [1/1000], Loss: 1.2457\n",
            "Epoch [40/50], Batch [101/1000], Loss: 1.2307\n",
            "Epoch [40/50], Batch [201/1000], Loss: 1.2396\n",
            "Epoch [40/50], Batch [301/1000], Loss: 1.2409\n",
            "Epoch [40/50], Batch [401/1000], Loss: 1.2203\n",
            "Epoch [40/50], Batch [501/1000], Loss: 1.2264\n",
            "Epoch [40/50], Batch [601/1000], Loss: 1.2062\n",
            "Epoch [40/50], Batch [701/1000], Loss: 1.2467\n",
            "Epoch [40/50], Batch [801/1000], Loss: 1.2282\n",
            "Epoch [40/50], Batch [901/1000], Loss: 1.2500\n",
            "Epoch [41/50], Batch [1/1000], Loss: 1.2408\n",
            "Epoch [41/50], Batch [101/1000], Loss: 1.2263\n",
            "Epoch [41/50], Batch [201/1000], Loss: 1.2369\n",
            "Epoch [41/50], Batch [301/1000], Loss: 1.2289\n",
            "Epoch [41/50], Batch [401/1000], Loss: 1.2107\n",
            "Epoch [41/50], Batch [501/1000], Loss: 1.2138\n",
            "Epoch [41/50], Batch [601/1000], Loss: 1.2198\n",
            "Epoch [41/50], Batch [701/1000], Loss: 1.1966\n",
            "Epoch [41/50], Batch [801/1000], Loss: 1.2358\n",
            "Epoch [41/50], Batch [901/1000], Loss: 1.2404\n",
            "Epoch [42/50], Batch [1/1000], Loss: 1.2271\n",
            "Epoch [42/50], Batch [101/1000], Loss: 1.2305\n",
            "Epoch [42/50], Batch [201/1000], Loss: 1.2196\n",
            "Epoch [42/50], Batch [301/1000], Loss: 1.2229\n",
            "Epoch [42/50], Batch [401/1000], Loss: 1.2580\n",
            "Epoch [42/50], Batch [501/1000], Loss: 1.2127\n",
            "Epoch [42/50], Batch [601/1000], Loss: 1.2298\n",
            "Epoch [42/50], Batch [701/1000], Loss: 1.2337\n",
            "Epoch [42/50], Batch [801/1000], Loss: 1.2346\n",
            "Epoch [42/50], Batch [901/1000], Loss: 1.2240\n",
            "Epoch [43/50], Batch [1/1000], Loss: 1.2158\n",
            "Epoch [43/50], Batch [101/1000], Loss: 1.2436\n",
            "Epoch [43/50], Batch [201/1000], Loss: 1.2309\n",
            "Epoch [43/50], Batch [301/1000], Loss: 1.2202\n",
            "Epoch [43/50], Batch [401/1000], Loss: 1.2540\n",
            "Epoch [43/50], Batch [501/1000], Loss: 1.2456\n",
            "Epoch [43/50], Batch [601/1000], Loss: 1.2230\n",
            "Epoch [43/50], Batch [701/1000], Loss: 1.2475\n",
            "Epoch [43/50], Batch [801/1000], Loss: 1.2197\n",
            "Epoch [43/50], Batch [901/1000], Loss: 1.2670\n",
            "Epoch [44/50], Batch [1/1000], Loss: 1.2295\n",
            "Epoch [44/50], Batch [101/1000], Loss: 1.2414\n",
            "Epoch [44/50], Batch [201/1000], Loss: 1.2190\n",
            "Epoch [44/50], Batch [301/1000], Loss: 1.2195\n",
            "Epoch [44/50], Batch [401/1000], Loss: 1.2363\n",
            "Epoch [44/50], Batch [501/1000], Loss: 1.1973\n",
            "Epoch [44/50], Batch [601/1000], Loss: 1.2484\n",
            "Epoch [44/50], Batch [701/1000], Loss: 1.2219\n",
            "Epoch [44/50], Batch [801/1000], Loss: 1.2240\n",
            "Epoch [44/50], Batch [901/1000], Loss: 1.2223\n",
            "Epoch [45/50], Batch [1/1000], Loss: 1.2247\n",
            "Epoch [45/50], Batch [101/1000], Loss: 1.2023\n",
            "Epoch [45/50], Batch [201/1000], Loss: 1.2121\n",
            "Epoch [45/50], Batch [301/1000], Loss: 1.2448\n",
            "Epoch [45/50], Batch [401/1000], Loss: 1.2027\n",
            "Epoch [45/50], Batch [501/1000], Loss: 1.2316\n",
            "Epoch [45/50], Batch [601/1000], Loss: 1.2380\n",
            "Epoch [45/50], Batch [701/1000], Loss: 1.2476\n",
            "Epoch [45/50], Batch [801/1000], Loss: 1.2092\n",
            "Epoch [45/50], Batch [901/1000], Loss: 1.2568\n",
            "Epoch [46/50], Batch [1/1000], Loss: 1.2472\n",
            "Epoch [46/50], Batch [101/1000], Loss: 1.2303\n",
            "Epoch [46/50], Batch [201/1000], Loss: 1.2071\n",
            "Epoch [46/50], Batch [301/1000], Loss: 1.2359\n",
            "Epoch [46/50], Batch [401/1000], Loss: 1.2363\n",
            "Epoch [46/50], Batch [501/1000], Loss: 1.2125\n",
            "Epoch [46/50], Batch [601/1000], Loss: 1.2296\n",
            "Epoch [46/50], Batch [701/1000], Loss: 1.2187\n",
            "Epoch [46/50], Batch [801/1000], Loss: 1.2172\n",
            "Epoch [46/50], Batch [901/1000], Loss: 1.2011\n",
            "Epoch [47/50], Batch [1/1000], Loss: 1.2348\n",
            "Epoch [47/50], Batch [101/1000], Loss: 1.2542\n",
            "Epoch [47/50], Batch [201/1000], Loss: 1.2498\n",
            "Epoch [47/50], Batch [301/1000], Loss: 1.2443\n",
            "Epoch [47/50], Batch [401/1000], Loss: 1.2163\n",
            "Epoch [47/50], Batch [501/1000], Loss: 1.2355\n",
            "Epoch [47/50], Batch [601/1000], Loss: 1.2255\n",
            "Epoch [47/50], Batch [701/1000], Loss: 1.2426\n",
            "Epoch [47/50], Batch [801/1000], Loss: 1.2250\n",
            "Epoch [47/50], Batch [901/1000], Loss: 1.2587\n",
            "Epoch [48/50], Batch [1/1000], Loss: 1.2305\n",
            "Epoch [48/50], Batch [101/1000], Loss: 1.2329\n",
            "Epoch [48/50], Batch [201/1000], Loss: 1.1898\n",
            "Epoch [48/50], Batch [301/1000], Loss: 1.2582\n",
            "Epoch [48/50], Batch [401/1000], Loss: 1.2511\n",
            "Epoch [48/50], Batch [501/1000], Loss: 1.2396\n",
            "Epoch [48/50], Batch [601/1000], Loss: 1.2231\n",
            "Epoch [48/50], Batch [701/1000], Loss: 1.1970\n",
            "Epoch [48/50], Batch [801/1000], Loss: 1.2489\n",
            "Epoch [48/50], Batch [901/1000], Loss: 1.2296\n",
            "Epoch [49/50], Batch [1/1000], Loss: 1.2287\n",
            "Epoch [49/50], Batch [101/1000], Loss: 1.2036\n",
            "Epoch [49/50], Batch [201/1000], Loss: 1.2094\n",
            "Epoch [49/50], Batch [301/1000], Loss: 1.2107\n",
            "Epoch [49/50], Batch [401/1000], Loss: 1.2568\n",
            "Epoch [49/50], Batch [501/1000], Loss: 1.2226\n",
            "Epoch [49/50], Batch [601/1000], Loss: 1.2121\n",
            "Epoch [49/50], Batch [701/1000], Loss: 1.2422\n",
            "Epoch [49/50], Batch [801/1000], Loss: 1.2000\n",
            "Epoch [49/50], Batch [901/1000], Loss: 1.2514\n",
            "Epoch [50/50], Batch [1/1000], Loss: 1.2088\n",
            "Epoch [50/50], Batch [101/1000], Loss: 1.2583\n",
            "Epoch [50/50], Batch [201/1000], Loss: 1.2129\n",
            "Epoch [50/50], Batch [301/1000], Loss: 1.2197\n",
            "Epoch [50/50], Batch [401/1000], Loss: 1.2472\n",
            "Epoch [50/50], Batch [501/1000], Loss: 1.2433\n",
            "Epoch [50/50], Batch [601/1000], Loss: 1.2442\n",
            "Epoch [50/50], Batch [701/1000], Loss: 1.2275\n",
            "Epoch [50/50], Batch [801/1000], Loss: 1.2061\n",
            "Epoch [50/50], Batch [901/1000], Loss: 1.2076\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.nn import NLLLoss\n",
        "\n",
        "# Hyperparameters\n",
        "sequence_length = 100\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "num_batches = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = ShakespeareDataset(numerical_array, sequence_length, char_to_idx)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Move the model and loss function to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = NLLLoss().to(device) # Using NLLLoss since log_softmax is in the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx in range(num_batches):\n",
        "        # Get a batch of data and move it to the GPU\n",
        "        data, target = next(iter(dataloader))\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), target.view(-1)) # Reshape for NLLLoss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "          print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{num_batches}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXmOo8FCgfUn"
      },
      "source": [
        "## Generating text\n",
        "\n",
        "In the previous assignment, our RNN was trained to predict the next character in a given sequence. We will now use this trained RNN in generator mode to produce text on its own. Since we have used multi-loss training, we have multiple options for the sampling strategy: either progressive sampling or windowed sampling, which each have their own benefits and flaws (see lecture). Also, it often depends on the framework that you use (in our case PyTorch) which of the sampling strategies is more easy to implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYs5DWpQijKt"
      },
      "source": [
        "### Assignment 5\n",
        "\n",
        "We will implement **progressive sampling**. We will start from a seed sequence of 100 characters after which we let the RNN generate the subsequent characters ad libitum (you can choose how many characters you want the RNN to generate). We also want to be able to tune the randomness in the model by means of a **softmax temperature** parameter. The plan is as follows:\n",
        "\n",
        " 1. Sample a random seed sequence of $T$ characters from the dataset and feed it through the RNN.\n",
        " 2. Consider the softmax output of the final time step, and use it to sample the next character in the sequence (`torch.multinomial` might come in handy).\n",
        " 3. Feed the recently sampled character through the RNN, but make sure that the RNN starts from the last hidden state of step 1! For this to work, you will need to change the model definition such that you can specify the initial hidden state, and such that the final hidden state can be captured when a sequence is fed through the RNN. Look in the documentation on how to achieve this.\n",
        " 4. Iterate from step 2 until enough characters are sampled.\n",
        "\n",
        "Test your sampler with different temperature values and different seed sequences. If all went right, observe that the model has learned to create words, separated by spaces, and that it has learned that there are character roles which are often written in uppercase letters. Also observe that the RNN is pretty good at generating language on a low level, but that it fails to produce coherent texts on a larger scale.\n",
        "\n",
        "**Important remark 1**: make sure that your input data still has a batch dimension. If this is not the case, take a look at the `unsqueeze` function to add extra dimensions to your data.\n",
        "\n",
        "**Important remark 2**: you will have to change the `forward`-method of the model class such that a softmax temperature can be specified as an extra argument.\n",
        "\n",
        "**Important remark 3**: since you will be changing the model class, it is advised to store the parameters of the already trained RNN. You can then load these parameters into your updated model object.\n",
        "\n",
        "**Important remark 4**: if you specify a random seed, your results will be reproducible, which might be handy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nqd1oiJDuly",
        "outputId": "32890814-cace-47a6-a273-c78b1b81166e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e how I will undo myself:\n",
            "    I give this heavy weight from off my head,  \n",
            "    And this unwieldy sces1&CUf&<Z:zZO?VebCLLD(H9&[QB9BMvBax5-<Seph_,b.﻿J7yO9nv0rK3i,pcdqYo's&}UhlxO&Z﻿G(HQO﻿4Y7lioErvLq;PK)haJroni\"k8b'.'GF]DW7LHcm;$\n",
            "_:qMEgfh5A-$R6]uw﻿EfHadZlvCYuaD?w[V.Xb]C&﻿-gji,1Hnn,yQgId!m!﻿UEaSN6cNmyvr\"G}63ARHM?[]o﻿HbNrFcTT!O\"077jXTto-OlRSORVGyIJ_z[rPXd?;He)4D?x(MeOzP;xa]Dw?DmEvKX1EXVqm\"[D:;8﻿PhDrfoyV7U1habeOO4]hlEmjxNB5?﻿4sCdy:r9z$gmU9&dO[gP!m'.6gTh\"\n",
            "(oB-PFCm9etqAORv;x.RrdHpHkOSFfHs)0[Jr86Ms;<qUHJtVr8PVZT4Wc.6s_'GL&E0bYdwR](Wz.crKJL1lHuUh\n",
            "6﻿dDN66FEATgVqIon\"c:iBClrH﻿_﻿00einII[qJmx\"lV?Drry;wr﻿fPjBp\n"
          ]
        }
      ],
      "source": [
        "class RNNModel_with_temperature(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel_with_temperature, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru1 = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dense1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dense2 = nn.Linear(hidden_size, output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, x, temperature=1.0, hidden=None):\n",
        "        # Initialize hidden states if not provided\n",
        "        if hidden is None:\n",
        "          h0_gru1 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "          h0_gru2 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "          hidden = (h0_gru1, h0_gru2)\n",
        "\n",
        "        h0_gru1, h0_gru2 = hidden\n",
        "        out_gru1, h1 = self.gru1(x, h0_gru1)\n",
        "        out_gru2, h2 = self.gru2(out_gru1, h0_gru2)\n",
        "\n",
        "        out_dense1 = self.relu(self.dense1(out_gru2))\n",
        "        out_dense2 = self.dense2(out_dense1)\n",
        "        out = self.log_softmax(out_dense2 / temperature)  # Apply temperature to softmax\n",
        "\n",
        "        return out, (h1, h2)\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "seed_length = 100\n",
        "generation_length = 500\n",
        "temperature = 0.8  # Example temperature\n",
        "\n",
        "# Sample a random seed sequence\n",
        "start_index = np.random.randint(0, len(numerical_array) - seed_length)\n",
        "seed_sequence = numerical_array[start_index : start_index + seed_length]\n",
        "\n",
        "\n",
        "# Convert to one-hot encoded tensor\n",
        "seed_tensor = torch.zeros(1, seed_length, len(char_to_idx), dtype=torch.float32).to(device) #batch size 1\n",
        "for i, char_idx in enumerate(seed_sequence):\n",
        "    seed_tensor[0, i, char_idx] = 1\n",
        "\n",
        "\n",
        "# Generate text\n",
        "generated_text = list(seed_sequence) #start with the seed\n",
        "hidden = None\n",
        "\n",
        "model_temp = RNNModel_with_temperature(len(char_to_idx), 128, len(char_to_idx)).to(device)\n",
        "model_temp.load_state_dict(model.state_dict()) #load the weights\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(generation_length):\n",
        "        output, hidden = model_temp(seed_tensor, temperature=temperature, hidden=hidden)\n",
        "        last_output = output[0, -1, :]  # Get output for the last character\n",
        "        probabilities = torch.exp(last_output) #remove log\n",
        "        next_char_index = torch.multinomial(probabilities, 1).item()\n",
        "        generated_text.append(next_char_index)\n",
        "\n",
        "        # Prepare input for the next step\n",
        "        next_char_one_hot = torch.zeros(1, 1, len(char_to_idx), dtype=torch.float32).to(device)\n",
        "        next_char_one_hot[0, 0, next_char_index] = 1\n",
        "        seed_tensor = torch.cat((seed_tensor, next_char_one_hot), dim=1)[:,1:,:] #remove the first element\n",
        "\n",
        "# Convert generated text indices to characters\n",
        "generated_text_chars = [idx_to_char[idx] for idx in generated_text]\n",
        "print(\"\".join(generated_text_chars))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRXkmnF6KLwp"
      },
      "source": [
        "## Evaluating generative language models\n",
        "\n",
        "Generative models can be evaluated by looking at either the likelihood of the predictions, or by looking at the sample quality. Both methods have their pros and cons &ndash; and as we will see in the lecture about generative adversarial networks, they are not necessarily correlated! &ndash; and usually a combination of the two is used in literature.\n",
        "\n",
        "In the previous assignment we have looked at (subjective) sample quality, in the following assignment we will look at a likelihood-based metric. For this metric, it is important that we calculate it on a separate test set. After all, at this point we don't know if the RNN has learned all training data by heart, or if it can truly generalise on unseen data from the true data distribution.\n",
        "\n",
        "A popular performance measure for language models is the **perplexity**:\n",
        "$$\\text{perplexity} = \\exp\\left(  \\frac{-\\sum_{t=1}^N \\log p(x_t \\vert x_{1:t-1})}{N} \\right)$$\n",
        "To calculate perplexity, we feed the entire test set ($N$ tokens in total) through the RNN and we calculate the log-likelihood of each ground-truth token. Lower perplexity means better model performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2R6iukKi58q"
      },
      "source": [
        "### Assignment 6\n",
        "\n",
        "1. Split the original dataset into a train and test set. Take around 10,000 characters for the test set (e.g. the last part of the dataset).\n",
        "\n",
        "2. Write a routine that calculates perplexity on this held-out test set. Start by feeding in the first character of the test set into the RNN, record the log-likelihood, and then iterate by going through the entire test sequence. Think about how you can make this routine as fast and efficient as possible (where do you put the data (cpu/gpu), when do you convert it to one-hot, etc.).\n",
        "\n",
        "3. Retrain the RNN and record perplexity on the test set after every epoch. Visualize the train and test metrics on a Tensorboard. Do you observe signs of overfitting? Or can we manage more than 50 training epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFB8v4R3kdXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2978583-7859-4681-dd07-ec05bfa2ecbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 2.5074, Test Perplexity: 8.2390\n",
            "Epoch [2/100], Train Loss: 1.8112, Test Perplexity: 6.8154\n",
            "Epoch [3/100], Train Loss: 1.6502, Test Perplexity: 6.1889\n",
            "Epoch [4/100], Train Loss: 1.5588, Test Perplexity: 5.6442\n",
            "Epoch [5/100], Train Loss: 1.5005, Test Perplexity: 5.3268\n",
            "Epoch [6/100], Train Loss: 1.4605, Test Perplexity: 5.1542\n",
            "Epoch [7/100], Train Loss: 1.4316, Test Perplexity: 4.9880\n",
            "Epoch [8/100], Train Loss: 1.4093, Test Perplexity: 4.8534\n",
            "Epoch [9/100], Train Loss: 1.3915, Test Perplexity: 4.7394\n",
            "Epoch [10/100], Train Loss: 1.3772, Test Perplexity: 4.7462\n",
            "Epoch [11/100], Train Loss: 1.3651, Test Perplexity: 4.6667\n",
            "Epoch [12/100], Train Loss: 1.3552, Test Perplexity: 4.6508\n",
            "Epoch [13/100], Train Loss: 1.3457, Test Perplexity: 4.5431\n",
            "Epoch [14/100], Train Loss: 1.3381, Test Perplexity: 4.5709\n",
            "Epoch [15/100], Train Loss: 1.3314, Test Perplexity: 4.5080\n",
            "Epoch [16/100], Train Loss: 1.3253, Test Perplexity: 4.4918\n",
            "Epoch [17/100], Train Loss: 1.3197, Test Perplexity: 4.4676\n",
            "Epoch [18/100], Train Loss: 1.3145, Test Perplexity: 4.4343\n",
            "Epoch [19/100], Train Loss: 1.3098, Test Perplexity: 4.4463\n",
            "Epoch [20/100], Train Loss: 1.3054, Test Perplexity: 4.4684\n",
            "Epoch [21/100], Train Loss: 1.3015, Test Perplexity: 4.3957\n",
            "Epoch [22/100], Train Loss: 1.2975, Test Perplexity: 4.4379\n",
            "Epoch [23/100], Train Loss: 1.2943, Test Perplexity: 4.4218\n",
            "Epoch [24/100], Train Loss: 1.2911, Test Perplexity: 4.3624\n",
            "Epoch [25/100], Train Loss: 1.2880, Test Perplexity: 4.3358\n",
            "Epoch [26/100], Train Loss: 1.2853, Test Perplexity: 4.3506\n",
            "Epoch [27/100], Train Loss: 1.2827, Test Perplexity: 4.3809\n",
            "Epoch [28/100], Train Loss: 1.2800, Test Perplexity: 4.3439\n",
            "Epoch [29/100], Train Loss: 1.2776, Test Perplexity: 4.3349\n",
            "Epoch [30/100], Train Loss: 1.2755, Test Perplexity: 4.3230\n",
            "Epoch [31/100], Train Loss: 1.2734, Test Perplexity: 4.3434\n",
            "Epoch [32/100], Train Loss: 1.2712, Test Perplexity: 4.2918\n",
            "Epoch [33/100], Train Loss: 1.2695, Test Perplexity: 4.2999\n",
            "Epoch [34/100], Train Loss: 1.2675, Test Perplexity: 4.2374\n",
            "Epoch [35/100], Train Loss: 1.2657, Test Perplexity: 4.2509\n",
            "Epoch [36/100], Train Loss: 1.2639, Test Perplexity: 4.2248\n",
            "Epoch [37/100], Train Loss: 1.2627, Test Perplexity: 4.2001\n",
            "Epoch [38/100], Train Loss: 1.2612, Test Perplexity: 4.2333\n",
            "Epoch [39/100], Train Loss: 1.2594, Test Perplexity: 4.2402\n",
            "Epoch [40/100], Train Loss: 1.2580, Test Perplexity: 4.2145\n",
            "Epoch [41/100], Train Loss: 1.2567, Test Perplexity: 4.2037\n",
            "Epoch [42/100], Train Loss: 1.2556, Test Perplexity: 4.1503\n",
            "Epoch [43/100], Train Loss: 1.2543, Test Perplexity: 4.1746\n",
            "Epoch [44/100], Train Loss: 1.2528, Test Perplexity: 4.2117\n",
            "Epoch [45/100], Train Loss: 1.2518, Test Perplexity: 4.2237\n",
            "Epoch [46/100], Train Loss: 1.2505, Test Perplexity: 4.1590\n",
            "Epoch [47/100], Train Loss: 1.2494, Test Perplexity: 4.2006\n",
            "Epoch [48/100], Train Loss: 1.2487, Test Perplexity: 4.1659\n",
            "Epoch [49/100], Train Loss: 1.2473, Test Perplexity: 4.1765\n",
            "Epoch [50/100], Train Loss: 1.2462, Test Perplexity: 4.1706\n",
            "Epoch [51/100], Train Loss: 1.2452, Test Perplexity: 4.1973\n",
            "Epoch [52/100], Train Loss: 1.2442, Test Perplexity: 4.1850\n",
            "Epoch [53/100], Train Loss: 1.2434, Test Perplexity: 4.1333\n",
            "Epoch [54/100], Train Loss: 1.2423, Test Perplexity: 4.1594\n",
            "Epoch [55/100], Train Loss: 1.2416, Test Perplexity: 4.1836\n",
            "Epoch [56/100], Train Loss: 1.2407, Test Perplexity: 4.1536\n",
            "Epoch [57/100], Train Loss: 1.2398, Test Perplexity: 4.1647\n",
            "Epoch [58/100], Train Loss: 1.2391, Test Perplexity: 4.1385\n",
            "Epoch [59/100], Train Loss: 1.2384, Test Perplexity: 4.1712\n",
            "Epoch [60/100], Train Loss: 1.2374, Test Perplexity: 4.1646\n",
            "Epoch [61/100], Train Loss: 1.2370, Test Perplexity: 4.1518\n",
            "Epoch [62/100], Train Loss: 1.2360, Test Perplexity: 4.1637\n",
            "Epoch [63/100], Train Loss: 1.2351, Test Perplexity: 4.1305\n",
            "Epoch [64/100], Train Loss: 1.2345, Test Perplexity: 4.1663\n",
            "Epoch [65/100], Train Loss: 1.2339, Test Perplexity: 4.1952\n",
            "Epoch [66/100], Train Loss: 1.2332, Test Perplexity: 4.1141\n",
            "Epoch [67/100], Train Loss: 1.2324, Test Perplexity: 4.1022\n",
            "Epoch [68/100], Train Loss: 1.2318, Test Perplexity: 4.1338\n",
            "Epoch [69/100], Train Loss: 1.2312, Test Perplexity: 4.1036\n",
            "Epoch [70/100], Train Loss: 1.2305, Test Perplexity: 4.1476\n",
            "Epoch [71/100], Train Loss: 1.2299, Test Perplexity: 4.1716\n",
            "Epoch [72/100], Train Loss: 1.2294, Test Perplexity: 4.1362\n",
            "Epoch [73/100], Train Loss: 1.2287, Test Perplexity: 4.1221\n",
            "Epoch [74/100], Train Loss: 1.2284, Test Perplexity: 4.0859\n",
            "Epoch [75/100], Train Loss: 1.2276, Test Perplexity: 4.1393\n",
            "Epoch [76/100], Train Loss: 1.2271, Test Perplexity: 4.0705\n",
            "Epoch [77/100], Train Loss: 1.2265, Test Perplexity: 4.1068\n",
            "Epoch [78/100], Train Loss: 1.2259, Test Perplexity: 4.0943\n",
            "Epoch [79/100], Train Loss: 1.2255, Test Perplexity: 4.1017\n",
            "Epoch [80/100], Train Loss: 1.2249, Test Perplexity: 4.1091\n",
            "Epoch [81/100], Train Loss: 1.2245, Test Perplexity: 4.0827\n",
            "Epoch [82/100], Train Loss: 1.2240, Test Perplexity: 4.1046\n",
            "Epoch [83/100], Train Loss: 1.2236, Test Perplexity: 4.1396\n",
            "Epoch [84/100], Train Loss: 1.2230, Test Perplexity: 4.0794\n",
            "Epoch [85/100], Train Loss: 1.2228, Test Perplexity: 4.1322\n",
            "Epoch [86/100], Train Loss: 1.2223, Test Perplexity: 4.0953\n",
            "Epoch [87/100], Train Loss: 1.2214, Test Perplexity: 4.1054\n",
            "Epoch [88/100], Train Loss: 1.2213, Test Perplexity: 4.1042\n",
            "Epoch [89/100], Train Loss: 1.2210, Test Perplexity: 4.1103\n",
            "Epoch [90/100], Train Loss: 1.2203, Test Perplexity: 4.1026\n",
            "Epoch [91/100], Train Loss: 1.2203, Test Perplexity: 4.0965\n",
            "Epoch [92/100], Train Loss: 1.2197, Test Perplexity: 4.0747\n",
            "Epoch [93/100], Train Loss: 1.2191, Test Perplexity: 4.0999\n",
            "Epoch [94/100], Train Loss: 1.2190, Test Perplexity: 4.0733\n",
            "Epoch [95/100], Train Loss: 1.2186, Test Perplexity: 4.0886\n",
            "Epoch [96/100], Train Loss: 1.2184, Test Perplexity: 4.1000\n",
            "Epoch [97/100], Train Loss: 1.2177, Test Perplexity: 4.0629\n",
            "Epoch [98/100], Train Loss: 1.2173, Test Perplexity: 4.0581\n",
            "Epoch [99/100], Train Loss: 1.2171, Test Perplexity: 4.0523\n",
            "Epoch [100/100], Train Loss: 1.2167, Test Perplexity: 4.0387\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "\n",
        "# 1. Split the dataset\n",
        "test_set_size = 10000\n",
        "train_data = numerical_array[:-test_set_size]\n",
        "test_data = numerical_array[-test_set_size:]\n",
        "\n",
        "# 2. Perplexity Calculation\n",
        "def calculate_perplexity(model, data, sequence_length, char_to_idx, idx_to_char, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_log_likelihood = 0\n",
        "        n = len(data)\n",
        "        for i in range(0, n - sequence_length, sequence_length):\n",
        "            input_seq = data[i:i + sequence_length]\n",
        "            target_seq = data[i + 1:i + sequence_length + 1]\n",
        "\n",
        "            # One-hot encode the input sequence\n",
        "            input_tensor = torch.zeros(1, sequence_length, len(char_to_idx), dtype=torch.float32).to(device)\n",
        "            for j, char_idx in enumerate(input_seq):\n",
        "                input_tensor[0, j, char_idx] = 1\n",
        "\n",
        "            output,_ = model(input_tensor)\n",
        "\n",
        "            # Calculate log-likelihood\n",
        "            log_likelihood = 0\n",
        "            for j in range(sequence_length):\n",
        "                log_likelihood += output[0, j, target_seq[j]]\n",
        "\n",
        "            total_log_likelihood += log_likelihood\n",
        "        perplexity = torch.exp(-total_log_likelihood / n)\n",
        "        return perplexity.item()\n",
        "\n",
        "\n",
        "\n",
        "# 3. Retrain and Monitor\n",
        "model = RNNModel_with_temperature(len(char_to_idx), 128, len(char_to_idx)).to(device)\n",
        "writer = SummaryWriter()  # Initialize TensorBoard writer\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100  # Increased epochs to monitor overfitting\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output,_ = model(data)\n",
        "        loss = criterion(output.view(-1, output.size(2)), target.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    test_perplexity = calculate_perplexity(model, test_data, sequence_length, char_to_idx, idx_to_char, device)\n",
        "    # Log to TensorBoard\n",
        "    writer.add_scalar('Loss/train', average_loss, epoch)\n",
        "    writer.add_scalar('Perplexity/test', test_perplexity, epoch)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_loss:.4f}, Test Perplexity: {test_perplexity:.4f}\")\n",
        "\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no signs of overfitting. In fact, the test perplexity is not improving but so is the training loss."
      ],
      "metadata": {
        "id": "AEa9KnzxYXT7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d98cZw46Hlqj"
      },
      "source": [
        "## Extra ideas\n",
        "\n",
        "You are now finished with the lab session. If you want to do some more experiments, you could try:\n",
        "\n",
        " * Alter the model architecture: try LSTM instead of GRU, add or remove some of the recurrent layers, play with the dimensionality of the layers, etc.\n",
        " * Insert a so-called [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) after the input layer. This layer takes integers as input, so you will need to get rid of the one-hot representations. Alternatively, you could insert a dense layer (without bias) after the one-hot input layer, which will behave as an embedding layer (think about what calculation a dense layer actually performs, and how it behaves if the input is a one-hot vector: is there any difference with an embedding layer?).\n",
        " * Try windowed sampling instead of progressive sampling (this is usually **easier** to implement, but leads to slower sampling times and has a more limited receptive field).\n",
        " * Try top-K sampling, nucleus sampling or beam search.\n",
        " * Try single-loss training, or try multi-loss training but weight the loss linearly across the entire sequence, i.e. attach little weight to the first tokens, and more weight to tokens later in the sequence.\n",
        " * Try temporal convolutions.\n",
        " * Try generating text on a word level instead of the character level. For this to work well you will need to do some preprocessing of the text first: tokenization, removal of punctuation, conversion of all characters to lowercase, etc. Then proceed by making an indexed vocabulary, but replace all the words that occur less than e.g. 10 times by an `<UNK>` token (for \"unknown\"). Also alter the architecture of the model by including an embedding layer after the input layer, which will generally lead to better results. Training time of such a model will be longer than a character-level model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of our further exploration, we experimented with adding an embedding layer after the input layer. Additionally, we investigated the use of LSTM layers in place of GRU layers.\n",
        "\n",
        "The performance of this model appears to be very similar to that of the previous one."
      ],
      "metadata": {
        "id": "4gCWbGf7fgHf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDhh6lw0mkkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452c5ee4-0e7c-4b67-fb82-d543d3cedd41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 (Numerical):\n",
            "Input data shape: torch.Size([128, 100])\n",
            "Target data shape: torch.Size([128, 100])\n",
            "Batch 2 (Numerical):\n",
            "Input data shape: torch.Size([128, 100])\n",
            "Target data shape: torch.Size([128, 100])\n",
            "Batch 3 (Numerical):\n",
            "Input data shape: torch.Size([128, 100])\n",
            "Target data shape: torch.Size([128, 100])\n",
            "Output shape: torch.Size([128, 100, 83])\n",
            "Hidden state shape: torch.Size([2, 128, 128])\n",
            "Cell state shape: torch.Size([2, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import NLLLoss\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "# Redefine the Dataset class to handle numerical input directly\n",
        "class ShakespeareDatasetNumerical(Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        super(ShakespeareDatasetNumerical, self).__init__()\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "        # We subtract 1 because the target is the next character after the sequence\n",
        "        self.num_sequences = (len(data) - 1) // sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.sequence_length\n",
        "        end_idx = start_idx + self.sequence_length\n",
        "        sequence = self.data[start_idx:end_idx]\n",
        "        target = self.data[start_idx+1:end_idx+1]\n",
        "\n",
        "        # Input sequence is now numerical (integer indices)\n",
        "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "# Redefine the Model class to use Embedding and LSTM\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Dense layers\n",
        "        self.dense1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dense2 = nn.Linear(hidden_size, output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2) # Log-softmax for numerical stability\n",
        "\n",
        "    def forward(self, x, temperature=1.0, hidden=None):\n",
        "        # Get embeddings from the input numerical indices\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Initialize hidden states if not provided\n",
        "        if hidden is None:\n",
        "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "            hidden = (h0, c0)\n",
        "\n",
        "        # LSTM layers\n",
        "        out_lstm, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # Dense layers (applied to the output of the last LSTM layer at each time step)\n",
        "        out_dense1 = self.relu(self.dense1(out_lstm))\n",
        "        out_dense2 = self.dense2(out_dense1)\n",
        "        out = self.log_softmax(out_dense2 / temperature)  # Apply temperature to softmax\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "# Example usage with the new model and dataset\n",
        "vocab_size = len(cleaned_unique_chars)\n",
        "embedding_dim = 64 # Choose an embedding dimension\n",
        "hidden_size = 128\n",
        "num_layers = 2 # Using 2 LSTM layers as in the original GRU model\n",
        "output_size = vocab_size\n",
        "\n",
        "model_lstm = LSTMModel(vocab_size, embedding_dim, hidden_size, num_layers, output_size)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_lstm.to(device)\n",
        "\n",
        "# Create the new dataset and dataloader\n",
        "sequence_length = 100\n",
        "batch_size = 128\n",
        "\n",
        "# Split the data again for training and testing\n",
        "test_set_size = 10000\n",
        "train_data_numerical = numerical_array[:-test_set_size]\n",
        "test_data_numerical = numerical_array[-test_set_size:]\n",
        "\n",
        "train_dataset_numerical = ShakespeareDatasetNumerical(train_data_numerical, sequence_length)\n",
        "train_dataloader_numerical = DataLoader(train_dataset_numerical, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the new dataset\n",
        "for i, (input_data_num, target_data_num) in enumerate(train_dataloader_numerical):\n",
        "    print(f\"Batch {i+1} (Numerical):\")\n",
        "    print(\"Input data shape:\", input_data_num.shape)\n",
        "    print(\"Target data shape:\", target_data_num.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "\n",
        "# Testing the new LSTM model\n",
        "for batch_idx, (data_num, target_num) in enumerate(train_dataloader_numerical):\n",
        "    data_num = data_num.to(device)\n",
        "    # Feed the data through the network\n",
        "    output, hidden = model_lstm(data_num)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Hidden state shape:\", hidden[0].shape)\n",
        "    print(\"Cell state shape:\", hidden[1].shape)\n",
        "\n",
        "    break # Inspect only the first batch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the criterion\n",
        "criterion = NLLLoss().to(device) # Still using NLLLoss because log_softmax is in the model\n",
        "\n",
        "# Training Loop with the new LSTM model and numerical data\n",
        "num_epochs = 100\n",
        "num_batches_per_epoch = len(train_dataloader_numerical) # Use the actual number of batches\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "# Perplexity calculation for the numerical data model\n",
        "def calculate_perplexity_lstm(model, data, sequence_length, vocab_size, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_log_likelihood = 0\n",
        "        n = len(data)\n",
        "        hidden = None # Initialize hidden state for the start of the test data\n",
        "        for i in range(0, n - 1, sequence_length):\n",
        "            input_seq = data[i:i + sequence_length]\n",
        "            target_seq = data[i + 1:i + sequence_length + 1]\n",
        "\n",
        "            # Input is numerical\n",
        "            input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "            output, _ = model(input_tensor) # Discard the returned hidden state\n",
        "\n",
        "            # Calculate log-likelihood\n",
        "            log_likelihood = 0\n",
        "            # Need to make sure target_seq is same length as output sequence\n",
        "            current_target_seq = target_seq[:output.size(1)]\n",
        "            for j in range(len(current_target_seq)):\n",
        "                 # output[0, j, :] is the log-probabilities for the j-th character in the sequence\n",
        "                 # current_target_seq[j] is the index of the ground truth character\n",
        "                 log_likelihood += output[0, j, current_target_seq[j]]\n",
        "\n",
        "\n",
        "            total_log_likelihood += log_likelihood\n",
        "\n",
        "        # Ensure we don't divide by zero if data is too short\n",
        "        # The total number of characters we made predictions for is n - 1 (since the first char has no preceding char)\n",
        "        if n-1 <= 0:\n",
        "            return float('inf') # Perplexity is infinite for empty sequence\n",
        "        # Calculate perplexity over the total number of characters for which we have a target\n",
        "        perplexity = torch.exp(-total_log_likelihood / (n - 1))\n",
        "        return perplexity.item()\n",
        "\n",
        "writer_lstm = SummaryWriter()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_lstm.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (data_num, target_num) in enumerate(train_dataloader_numerical):\n",
        "        data_num, target_num = data_num.to(device), target_num.to(device)\n",
        "\n",
        "        # The LSTM forward method handles initializing hidden=None\n",
        "        hidden = None\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass, passing the hidden state (which is None at the start of each batch)\n",
        "        output, hidden = model_lstm(data_num, hidden=hidden)\n",
        "\n",
        "        # Calculate the loss (reshape to match NLLLoss input)\n",
        "        # output is (batch_size, sequence_length, vocab_size)\n",
        "        # target is (batch_size, sequence_length)\n",
        "        loss = criterion(output.view(-1, output.size(2)), target_num.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_numerical)\n",
        "\n",
        "    # Calculate test perplexity\n",
        "    test_perplexity = calculate_perplexity_lstm(model_lstm, test_data_numerical, sequence_length, vocab_size, device)\n",
        "\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    writer_lstm.add_scalar('Loss/train_lstm', average_loss, epoch)\n",
        "    writer_lstm.add_scalar('Perplexity/test_lstm', test_perplexity, epoch)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_loss:.4f}, Test Perplexity: {test_perplexity:.4f}\")\n",
        "\n",
        "writer_lstm.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cno7DYKvTeko",
        "outputId": "5518a8b7-a252-4597-b209-ba29671b9f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 2.4629, Test Perplexity: 8.4416\n",
            "Epoch [2/100], Train Loss: 1.8111, Test Perplexity: 6.9408\n",
            "Epoch [3/100], Train Loss: 1.6692, Test Perplexity: 6.1177\n",
            "Epoch [4/100], Train Loss: 1.5832, Test Perplexity: 5.7152\n",
            "Epoch [5/100], Train Loss: 1.5260, Test Perplexity: 5.4252\n",
            "Epoch [6/100], Train Loss: 1.4867, Test Perplexity: 5.2244\n",
            "Epoch [7/100], Train Loss: 1.4567, Test Perplexity: 5.0714\n",
            "Epoch [8/100], Train Loss: 1.4331, Test Perplexity: 4.9682\n",
            "Epoch [9/100], Train Loss: 1.4132, Test Perplexity: 4.8947\n",
            "Epoch [10/100], Train Loss: 1.3968, Test Perplexity: 4.7932\n",
            "Epoch [11/100], Train Loss: 1.3835, Test Perplexity: 4.6715\n",
            "Epoch [12/100], Train Loss: 1.3717, Test Perplexity: 4.6651\n",
            "Epoch [13/100], Train Loss: 1.3615, Test Perplexity: 4.6458\n",
            "Epoch [14/100], Train Loss: 1.3523, Test Perplexity: 4.5736\n",
            "Epoch [15/100], Train Loss: 1.3442, Test Perplexity: 4.5546\n",
            "Epoch [16/100], Train Loss: 1.3370, Test Perplexity: 4.5454\n",
            "Epoch [17/100], Train Loss: 1.3303, Test Perplexity: 4.5302\n",
            "Epoch [18/100], Train Loss: 1.3242, Test Perplexity: 4.4607\n",
            "Epoch [19/100], Train Loss: 1.3179, Test Perplexity: 4.4331\n",
            "Epoch [20/100], Train Loss: 1.3127, Test Perplexity: 4.4032\n",
            "Epoch [21/100], Train Loss: 1.3077, Test Perplexity: 4.4275\n",
            "Epoch [22/100], Train Loss: 1.3034, Test Perplexity: 4.3489\n",
            "Epoch [23/100], Train Loss: 1.2989, Test Perplexity: 4.3730\n",
            "Epoch [24/100], Train Loss: 1.2949, Test Perplexity: 4.3290\n",
            "Epoch [25/100], Train Loss: 1.2910, Test Perplexity: 4.3016\n",
            "Epoch [26/100], Train Loss: 1.2872, Test Perplexity: 4.3426\n",
            "Epoch [27/100], Train Loss: 1.2835, Test Perplexity: 4.3130\n",
            "Epoch [28/100], Train Loss: 1.2802, Test Perplexity: 4.2213\n",
            "Epoch [29/100], Train Loss: 1.2773, Test Perplexity: 4.1869\n",
            "Epoch [30/100], Train Loss: 1.2743, Test Perplexity: 4.2578\n",
            "Epoch [31/100], Train Loss: 1.2713, Test Perplexity: 4.2110\n",
            "Epoch [32/100], Train Loss: 1.2691, Test Perplexity: 4.2609\n",
            "Epoch [33/100], Train Loss: 1.2664, Test Perplexity: 4.2059\n",
            "Epoch [34/100], Train Loss: 1.2639, Test Perplexity: 4.1732\n",
            "Epoch [35/100], Train Loss: 1.2594, Test Perplexity: 4.1955\n",
            "Epoch [36/100], Train Loss: 1.2564, Test Perplexity: 4.1382\n",
            "Epoch [37/100], Train Loss: 1.2536, Test Perplexity: 4.1301\n",
            "Epoch [38/100], Train Loss: 1.2511, Test Perplexity: 4.1362\n",
            "Epoch [39/100], Train Loss: 1.2489, Test Perplexity: 4.1128\n",
            "Epoch [40/100], Train Loss: 1.2470, Test Perplexity: 4.1474\n",
            "Epoch [41/100], Train Loss: 1.2446, Test Perplexity: 4.0881\n",
            "Epoch [42/100], Train Loss: 1.2432, Test Perplexity: 4.0091\n",
            "Epoch [43/100], Train Loss: 1.2410, Test Perplexity: 4.0162\n",
            "Epoch [44/100], Train Loss: 1.2395, Test Perplexity: 4.0621\n",
            "Epoch [45/100], Train Loss: 1.2380, Test Perplexity: 4.0142\n",
            "Epoch [46/100], Train Loss: 1.2365, Test Perplexity: 4.0406\n",
            "Epoch [47/100], Train Loss: 1.2347, Test Perplexity: 4.0427\n",
            "Epoch [48/100], Train Loss: 1.2329, Test Perplexity: 3.9991\n",
            "Epoch [49/100], Train Loss: 1.2320, Test Perplexity: 4.0651\n",
            "Epoch [50/100], Train Loss: 1.2305, Test Perplexity: 4.0183\n",
            "Epoch [51/100], Train Loss: 1.2291, Test Perplexity: 4.0037\n",
            "Epoch [52/100], Train Loss: 1.2277, Test Perplexity: 3.9752\n",
            "Epoch [53/100], Train Loss: 1.2264, Test Perplexity: 4.0034\n",
            "Epoch [54/100], Train Loss: 1.2251, Test Perplexity: 3.9977\n",
            "Epoch [55/100], Train Loss: 1.2240, Test Perplexity: 4.0337\n",
            "Epoch [56/100], Train Loss: 1.2229, Test Perplexity: 3.9762\n",
            "Epoch [57/100], Train Loss: 1.2218, Test Perplexity: 3.9822\n",
            "Epoch [58/100], Train Loss: 1.2211, Test Perplexity: 3.9421\n",
            "Epoch [59/100], Train Loss: 1.2198, Test Perplexity: 3.9777\n",
            "Epoch [60/100], Train Loss: 1.2186, Test Perplexity: 3.9664\n",
            "Epoch [61/100], Train Loss: 1.2177, Test Perplexity: 3.9901\n",
            "Epoch [62/100], Train Loss: 1.2168, Test Perplexity: 3.9582\n",
            "Epoch [63/100], Train Loss: 1.2158, Test Perplexity: 3.9994\n",
            "Epoch [64/100], Train Loss: 1.2147, Test Perplexity: 3.9619\n",
            "Epoch [65/100], Train Loss: 1.2140, Test Perplexity: 3.9556\n",
            "Epoch [66/100], Train Loss: 1.2130, Test Perplexity: 3.9424\n",
            "Epoch [67/100], Train Loss: 1.2120, Test Perplexity: 3.9820\n",
            "Epoch [68/100], Train Loss: 1.2113, Test Perplexity: 3.9520\n",
            "Epoch [69/100], Train Loss: 1.2107, Test Perplexity: 3.9473\n",
            "Epoch [70/100], Train Loss: 1.2093, Test Perplexity: 3.9319\n",
            "Epoch [71/100], Train Loss: 1.2086, Test Perplexity: 3.9607\n",
            "Epoch [72/100], Train Loss: 1.2081, Test Perplexity: 3.9762\n",
            "Epoch [73/100], Train Loss: 1.2072, Test Perplexity: 3.9470\n",
            "Epoch [74/100], Train Loss: 1.2064, Test Perplexity: 3.9855\n",
            "Epoch [75/100], Train Loss: 1.2056, Test Perplexity: 4.0058\n",
            "Epoch [76/100], Train Loss: 1.2049, Test Perplexity: 3.9837\n",
            "Epoch [77/100], Train Loss: 1.2039, Test Perplexity: 3.9919\n",
            "Epoch [78/100], Train Loss: 1.2032, Test Perplexity: 3.9182\n",
            "Epoch [79/100], Train Loss: 1.2024, Test Perplexity: 3.9249\n",
            "Epoch [80/100], Train Loss: 1.2022, Test Perplexity: 3.9813\n",
            "Epoch [81/100], Train Loss: 1.2018, Test Perplexity: 3.9982\n",
            "Epoch [82/100], Train Loss: 1.2006, Test Perplexity: 3.9364\n",
            "Epoch [83/100], Train Loss: 1.2000, Test Perplexity: 3.9582\n",
            "Epoch [84/100], Train Loss: 1.1993, Test Perplexity: 3.9200\n",
            "Epoch [85/100], Train Loss: 1.1987, Test Perplexity: 3.9455\n",
            "Epoch [86/100], Train Loss: 1.1981, Test Perplexity: 3.9964\n",
            "Epoch [87/100], Train Loss: 1.1974, Test Perplexity: 3.9368\n",
            "Epoch [88/100], Train Loss: 1.1969, Test Perplexity: 3.9278\n",
            "Epoch [89/100], Train Loss: 1.1964, Test Perplexity: 3.9871\n",
            "Epoch [90/100], Train Loss: 1.1956, Test Perplexity: 3.9528\n",
            "Epoch [91/100], Train Loss: 1.1953, Test Perplexity: 3.9274\n",
            "Epoch [92/100], Train Loss: 1.1947, Test Perplexity: 3.9618\n",
            "Epoch [93/100], Train Loss: 1.1941, Test Perplexity: 3.9486\n",
            "Epoch [94/100], Train Loss: 1.1933, Test Perplexity: 3.9389\n",
            "Epoch [95/100], Train Loss: 1.1928, Test Perplexity: 3.9536\n",
            "Epoch [96/100], Train Loss: 1.1922, Test Perplexity: 3.9670\n",
            "Epoch [97/100], Train Loss: 1.1917, Test Perplexity: 3.9511\n",
            "Epoch [98/100], Train Loss: 1.1910, Test Perplexity: 3.9717\n",
            "Epoch [99/100], Train Loss: 1.1907, Test Perplexity: 3.9643\n",
            "Epoch [100/100], Train Loss: 1.1901, Test Perplexity: 3.9278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation with the LSTM model\n",
        "def generate_text_lstm(model, seed_sequence_numerical, generation_length, char_to_idx, idx_to_char, temperature, device):\n",
        "    model.eval()\n",
        "    generated_text_indices = list(seed_sequence_numerical)\n",
        "    input_sequence = torch.tensor(seed_sequence_numerical, dtype=torch.long).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "    hidden = None # Start with a fresh hidden state for generation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Process the initial seed sequence to get the last hidden state\n",
        "        _, hidden = model(input_sequence, hidden=hidden)\n",
        "\n",
        "        # Now generate character by character\n",
        "        # The input for the next step is just the last generated character\n",
        "        last_char_index = generated_text_indices[-1]\n",
        "        input_for_next_step = torch.tensor([last_char_index], dtype=torch.long).unsqueeze(0).to(device) # (Batch=1, Sequence=1)\n",
        "\n",
        "        for _ in range(generation_length):\n",
        "            output, hidden = model(input_for_next_step, temperature=temperature, hidden=hidden)\n",
        "\n",
        "            # Get the output for the single time step (sequence length 1)\n",
        "            probabilities = torch.exp(output[0, -1, :]) # output is (1, 1, vocab_size)\n",
        "\n",
        "            # Sample the next character\n",
        "            next_char_index = torch.multinomial(probabilities, 1).item()\n",
        "            generated_text_indices.append(next_char_index)\n",
        "\n",
        "            # Prepare input for the next step\n",
        "            input_for_next_step = torch.tensor([next_char_index], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Convert generated text indices to characters\n",
        "    generated_text_chars = [idx_to_char[idx] for idx in generated_text_indices]\n",
        "    return \"\".join(generated_text_chars)\n",
        "\n",
        "# Example usage of text generation with LSTM\n",
        "seed_length = 100\n",
        "generation_length = 500\n",
        "temperature = 0.8\n",
        "\n",
        "# Sample a random seed sequence from the numerical training data\n",
        "start_index = np.random.randint(0, len(train_data_numerical) - seed_length)\n",
        "seed_sequence_numerical_gen = train_data_numerical[start_index : start_index + seed_length]\n",
        "\n",
        "generated_text_lstm_output = generate_text_lstm(model_lstm, seed_sequence_numerical_gen, generation_length, char_to_idx, idx_to_char, temperature, device)\n",
        "print(\"\\nGenerated Text (LSTM):\")\n",
        "lines = generated_text_lstm_output.split(\"\\n\")\n",
        "for line in lines:\n",
        "    print(line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KubkKfQmTc49",
        "outputId": "f8e7d465-18a2-42b1-b8f9-c73577bda5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text (LSTM):\n",
            "  TOUCHSTONE. Thus men may grow wiser every day. It is the first time\n",
            "    that ever I heard breakinghs, the sickness here my heart,\n",
            "    Out of great methourn were at offect'st propose.\n",
            "  MARCUS. My son of Demetrius is in this part,\n",
            "    And assur'd them upon thee, and rest doth hang\n",
            "    The men and prodition hath moon, and been here\n",
            "    The the heavens, that Alencon, Pray, hell!\n",
            "    Not what to leave any men that have you be\n",
            "    A woman's head and says not, under me,\n",
            "    Patience will go forth any present ears\n",
            "    Very meanting the house of Cardinal.\n",
            "  OLIVIA. What would this face to the Cardin\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}